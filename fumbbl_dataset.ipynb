{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a public dataset of Blood Bowl matches played on FUMBBL.com\n",
    "\n",
    "This blogpost is about **Blood Bowl**, a boardgame I started playing last year. The goal of this blog post is to use Python API and HTML scraping to fetch online Blood Bowl match outcome data, and to create a structured dataset ready for analysis and visualization. This blogpost is written as a Jupyter notebook containing Python code, and is fully reproducible. The idea is to make Blood Bowl data analysis accessible to others. Using open source tooling reduces the barriers for others to build on other peopleâ€™s work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from isoweek import Week\n",
    "\n",
    "import requests # API library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Bowl online: FUMBBL \n",
    "\n",
    "The **FUMBBL** website (https://fumbbl.com) is one big pile of data. From coach pages, with their teams, to team rosters, with players, and match histories. It's all there.\n",
    "\n",
    "To obtain **FUMBBL** data, we need to fetch it match by match, team by team. To do so, the site creator Christer Kaivo-oja, from Sweden, has made an API that allows us to easily fetch data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python requests package to fetch data\n",
    "We use the [Python **Requests** library](https://docs.python-requests.org/en/latest/) to make the API call over HTTPS and obtain the response from the FUMBLL server. The response is in the JSON format, a [light-weight data-interchange format](https://www.json.org/json-en.html) which is both easy to read and write for humans, and easy to parse and generate by computers. So this makes it a natural choice for an API.\n",
    "The full documentation of the API can be found at (https://fumbbl.com/apidoc/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structure\n",
    "\n",
    "We go with a flat data frame with **rows for each match**, and columns for the various variables associated with each match.\n",
    "These would include:\n",
    "\n",
    "* Coach ids\n",
    "* Team races\n",
    "* Team ids\n",
    "* Date of the match\n",
    "* Outcome (Touchdowns of both teams)\n",
    "\n",
    "With this basic structure, we can add as many match related variables in the future, keeping the basic structure (each row is a match) unchanged.\n",
    "\n",
    "So lets get the match data!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: API scraping the match data: df_matches\n",
    "\n",
    "So we are mostly interested in the current ruleset, this is `BB2020`. This ruleset became available in **FUMBBL** at september 1st 2021, and two months later, some 5000 games have been played. We also want to compare with the previous ruleset, where we have much more data available. \n",
    "The dataset start with match `4216258` played on august 1st, 2020. This covers roughly 12 months of `BB2016` ruleset matches, after that it switches to predominantly `BB2020` matches.\n",
    "\n",
    "We collect match data by looping over `match_id`. We store the full JSON file on disk, so we avoid repeat API calls in the future.\n",
    "\n",
    "**VERY IMPORTANT: We do not want to overload the **FUMBBL** server, so we make only three API requests per second. In this way, the server load is hardly affected and it can continue functioning properly for all the Blood Bowl coaches playing their daily games!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/write_json_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_match_jsons(full_run, begin_match, end_match, verbose = False):\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "\n",
    "    print(\"matches to grab:\")\n",
    "    print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        for i in range(n_matches):\n",
    "            if i % 100 == 0: \n",
    "                if verbose:\n",
    "                    # write progress report\n",
    "                    print(i, end='')\n",
    "                    print(\".\")\n",
    "\n",
    "            match_id = end_match - i\n",
    "\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "            fname_string = dirname + \"/match_\" + str(match_id) + \".json\"\n",
    "\n",
    "            # check if file already exists, else scrape it\n",
    "            try:\n",
    "                f = open(fname_string, mode = \"rb\")\n",
    "\n",
    "            except OSError as e:\n",
    "              # scrape it\n",
    "                api_string = \"https://fumbbl.com/api/match/get/\" + str(match_id)\n",
    "\n",
    "                match = requests.get(api_string)\n",
    "                match = match.json()\n",
    "\n",
    "                write_json_file(match, fname_string)\n",
    "                if verbose:\n",
    "                    print(\"x\", end = '')\n",
    "                time.sleep(0.3)\n",
    "            else:\n",
    "                # file already present\n",
    "                if verbose:\n",
    "                    print(\"o\", end = '')\n",
    "                continue\n",
    "\n",
    "update_match_jsons(full_run = 0, begin_match = 4460944, end_match = 4474450, verbose=True) # takes 17s if no new matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the match JSONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_match_jsons(full_run, begin_match, end_match, verbose = True, target_file = None):\n",
    "\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "\n",
    "    print(\"matches to process:\")\n",
    "    print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        target = 'raw/df_matches_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "        print(target)\n",
    "\n",
    "        match_id_list = []\n",
    "        replay_id = []\n",
    "        tournament_id = []\n",
    "        division_id = []\n",
    "        division_name = []\n",
    "        scheduler = []\n",
    "        match_date = []\n",
    "        match_time = []\n",
    "        match_conceded = []\n",
    "        team1_id = []\n",
    "        team2_id = []\n",
    "        # touchdowns\n",
    "        team1_score = []\n",
    "        team2_score = []\n",
    "        # casualties\n",
    "        team1_cas_bh = []\n",
    "        team1_cas_si = []\n",
    "        team1_cas_rip = []\n",
    "        team2_cas_bh = []\n",
    "        team2_cas_si = []\n",
    "        team2_cas_rip = []\n",
    "        # other\n",
    "        team1_roster_id = []\n",
    "        team2_roster_id = []\n",
    "        team1_coach_id = []\n",
    "        team2_coach_id = []\n",
    "        team1_race_name = []\n",
    "        team2_race_name = []\n",
    "        team1_value = []\n",
    "        team2_value = []\n",
    "\n",
    "        for i in range(n_matches):\n",
    "            if i % 10000 == 0: \n",
    "                if verbose:\n",
    "                    # write progress report\n",
    "                    print(i, end='')\n",
    "                    print(\".\", end='')\n",
    "\n",
    "            match_id = end_match - i\n",
    "\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "            fname_string = dirname + \"/match_\" + str(match_id) + \".json\"\n",
    "             \n",
    "            \n",
    "            # read compressed json file\n",
    "            with open(fname_string, mode = \"rb\") as f:\n",
    "                match = json.load(f)\n",
    "\n",
    "            if match: # fix for matches that do not exist\n",
    "                match_id_list.append(match['id'])\n",
    "                replay_id.append(match['replayId'])\n",
    "                tournament_id.append(match['tournamentId']) # key to tournament table\n",
    "                division_id.append(match['divisionId'])\n",
    "                division_name.append(match['division'])\n",
    "                scheduler.append(match['scheduler'])\n",
    "                match_date.append(match['date'])\n",
    "                match_time.append(match['time'])\n",
    "                match_conceded.append(match['conceded'])\n",
    "                team1_id.append(match['team1']['id'])\n",
    "                team2_id.append(match['team2']['id'])\n",
    "                # touchdowns\n",
    "                team1_score.append(match['team1']['score'])\n",
    "                team2_score.append(match['team2']['score'] ) \n",
    "                # casualties\n",
    "                team1_cas_bh.append(match['team1']['casualties']['bh'])\n",
    "                team1_cas_si.append(match['team1']['casualties']['si'])\n",
    "                team1_cas_rip.append(match['team1']['casualties']['rip'])\n",
    "                team2_cas_bh.append(match['team2']['casualties']['bh'])\n",
    "                team2_cas_si.append(match['team2']['casualties']['si'])\n",
    "                team2_cas_rip.append(match['team2']['casualties']['rip'])\n",
    "                # other\n",
    "                team1_roster_id.append(match['team1']['roster']['id'])\n",
    "                team2_roster_id.append(match['team2']['roster']['id'] )           \n",
    "                team1_coach_id.append(match['team1']['coach']['id'])\n",
    "                team2_coach_id.append(match['team2']['coach']['id'])\n",
    "                team1_race_name.append(match['team1']['roster']['name'] )\n",
    "                team2_race_name.append(match['team2']['roster']['name'] )\n",
    "                team1_value.append(match['team1']['teamValue'])\n",
    "                team2_value.append(match['team2']['teamValue'])\n",
    "                \n",
    "            else:\n",
    "                # skip match\n",
    "                continue \n",
    "                \n",
    "        data = zip(match_id_list, replay_id, tournament_id, division_id, division_name, scheduler, match_date, match_time,  match_conceded, \n",
    "                   team1_id, team1_coach_id, team1_roster_id, team1_race_name, team1_value, team1_cas_bh, team1_cas_si, team1_cas_rip,\n",
    "                   team2_id, team2_coach_id, team2_roster_id, team2_race_name, team2_value, team2_cas_bh, team2_cas_si, team2_cas_rip,\n",
    "                   team1_score, team2_score)\n",
    "\n",
    "        df_matches = pd.DataFrame(data, columns=['match_id', 'replay_id', 'tournament_id', 'division_id', 'division_name', 'scheduler', 'match_date', 'match_time',  'match_conceded',\n",
    "        'team1_id', 'team1_coach_id', 'team1_roster_id', 'team1_race_name', 'team1_value', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "        'team2_id', 'team2_coach_id', 'team2_roster_id', 'team2_race_name', 'team2_value', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "        'team1_score', 'team2_score'])\n",
    "      \n",
    "        df_matches = df_matches.sort_values(by=['match_id']).reset_index(drop=True)\n",
    "        df_matches.to_hdf(target, key='df_matches', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file    \n",
    "        df_matches = pd.read_hdf(target_file) \n",
    "\n",
    "    return df_matches\n",
    "\n",
    "\n",
    "#df_matches = process_match_jsons(full_run = 1, begin_match = 4460944, end_match = 4474450, verbose= True)\n",
    "#df_matches = process_match_jsons(full_run = 0, begin_match = 4460944, end_match = 4474450, verbose= True, target_file = 'raw/df_matches_20230728_082435.h5')\n",
    "df_matches = process_match_jsons(full_run = 0, begin_match = 4216259, end_match = 4474450, verbose= True, target_file = 'raw/df_matches_20230728_215320.h5')\n",
    "#df_matches = process_match_jsons(full_run = 1, begin_match = 4216259, end_match = 4474450, verbose= True)\n",
    "\n",
    "# here we need as begin_match the very first match on disk\n",
    "\n",
    "df_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "258191-244646"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep: fixing the datatypes, creating derived variables\n",
    "\n",
    "Since we manually filled the `pandas` DataFrame, most of the columns are now of `object` datatype.\n",
    "We need to change this to be able to work properly with the data, as well as store it properly.\n",
    "Here I convert each column manually, however I later found out about `DataFrame.infer_objects()`, that can detect the proper dtype automatically.\n",
    "This I will try next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pm convert into function\n",
    "\n",
    "# convert object dtype columns to proper pandas dtypes datetime and numeric\n",
    "df_matches['match_date'] = pd.to_datetime(df_matches.match_date) # Datetime object\n",
    "\n",
    "# calculate match score difference\n",
    "df_matches['team1_win'] = np.sign(df_matches['team1_score'] - df_matches['team2_score'])\n",
    "df_matches['team2_win'] = np.sign(df_matches['team2_score'] - df_matches['team1_score'])\n",
    "\n",
    "# mirror match\n",
    "df_matches['mirror_match'] = 0\n",
    "df_matches.loc[df_matches['team1_race_name'] == df_matches['team2_race_name'], 'mirror_match'] = 1\n",
    "\n",
    "# add total CAS\n",
    "df_matches['team1_cas'] = df_matches['team1_cas_bh'] + df_matches['team1_cas_si'] + df_matches['team1_cas_rip']\n",
    "\n",
    "# add total CAS\n",
    "df_matches['team2_cas'] = df_matches['team2_cas_bh'] + df_matches['team2_cas_si'] + df_matches['team2_cas_rip']\n",
    "\n",
    "# mirror matches\n",
    "df_matches.query('mirror_match == 1').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: transforming the team values\n",
    "\n",
    "In Blood Bowl, teams can develop themselves over the course of multiple matches. The winnings of each match can be spend on buying new, stronger players, or replace the players that ended up getting injured or even killed. In addition, players receive so-called *star points* for important events, such as scoring, or inflicting a casualty on the opponent. Therefore, a balancing mechanism is needed when a newly created \"rookie\" team is facing a highly developed opposing team with lots of extra skills and strong players. \n",
    "\n",
    "Blood Bowl solves this by calculating for both teams their **Current team value**.\n",
    "The **Team value difference** for a match determines the amount of gold that the weaker team can use to buy so-called **inducements**.\n",
    "These inducements are temporary, and can consists of a famous \"star player\" who joins the team just for this match. Another popular option is to hire a wizard that can be used to turn one of the opposing players into a frog.\n",
    "\n",
    "It is well known that the win rates of the teams depend on how developed a team is. For example, Amazons are thought to be strongest at low team value, as they already start out with lots of *block* and *dodge* skills, whereas a Chaos team start out with almost no skills.\n",
    "So if we compare win rates, we would like take into account the current team value. \n",
    "Now as this can differ between the two teams in a match up, I reasoned that the highest team value is most informative about the average strength level of both teams, because of the inducement mechanism described above. (In the next step, we will add information on inducements)\n",
    "\n",
    "In the dataset, we have for each match the current team values of both teams as a text string. \n",
    "We transform the text string `1100k` into an integer number `1100`, so that we can calculated the difference as `tv_diff`, and pick for each match the maximum team value and store it as `tv_match`. Finally, we create a team value bin `tv_bin` to be able to compare win rates for binned groups of matches where races have comparable team strength / team development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PM make function transform_data\n",
    "def transform_data():\n",
    "    # do nothing\n",
    "    print(\"\")\n",
    "\n",
    "# convert team value 1100k to 1100 integer and and above / below median (= low / high TV)\n",
    "df_matches['team1_value'] = df_matches['team1_value'].str.replace('k$', '')\n",
    "df_matches['team1_value'] = df_matches['team1_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['team2_value'] = df_matches['team2_value'].str.replace('k$', '')\n",
    "df_matches['team2_value'] = df_matches['team2_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['tv_diff'] = np.abs(df_matches['team2_value'] - df_matches['team1_value'])\n",
    "df_matches['tv_diff2'] = df_matches['team2_value'] - df_matches['team1_value']\n",
    "\n",
    "df_matches['tv_match'] = df_matches[[\"team1_value\", \"team2_value\"]].max(axis=1)\n",
    "\n",
    "df_matches['tv_bin'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1250,1550, 1850, float(\"inf\")], \n",
    "    labels=['< 950K', '1.1M', '1.4M', '1.7M', '> 1850K']\n",
    ")\n",
    "\n",
    "df_matches['tv_bin2'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1050, 1150, 1250, 1350, 1450, 1550, float(\"inf\")], \n",
    "    labels=['< 950K', '1.0M', '1.1M', '1.2M',  '1.3M', '1.4M', '1.5M', '> 1550K']\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping empty matches\n",
    "\n",
    "Some match_id's do not have match information attached to them, presumably these matches were not played or some real life event interfered. These match_ids are dropped from the dataset to get rid of the NAs in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_matches)\n",
    "df_matches = df_matches.dropna(subset=['match_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: getting the dates right\n",
    "\n",
    "To see time trends, its useful to aggregate the data by week. For this we add `week_number` for each date, and from this week number, we convert back to a date to get a `week_date`. This last part is useful for plotting with `plotnine`, as this treats dates in a special way\n",
    "We use the ISO definition of week, this has some unexpected behavior near the beginning / end of each year. \n",
    "\n",
    "The data starts in week 36 (september) of 2020, and stops halfway march 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['week_number'] = df_matches['match_date'].dt.isocalendar().week\n",
    "\n",
    "# cannot serialize numpy int OR Int64 when writing HDF5 file, so we go for plain int as all NAs are gone now\n",
    "df_matches['week_number'] = df_matches['week_number'].fillna(0).astype(int)\n",
    "\n",
    "# add year based on match ISO week\n",
    "df_matches['year'] = df_matches['match_date'].dt.isocalendar().year.astype(int)\n",
    "\n",
    "df_matches['week_year'] = df_matches['year'].astype(str) + '-' + df_matches['week_number'].astype(str)\n",
    "\n",
    "# use a lambda function since isoweek.Week is not vectorized \n",
    "df_matches['week_date'] = pd.to_datetime(df_matches.apply(lambda row : Week(int(row[\"year\"]),int(row[\"week_number\"])).monday(),axis=1))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: HTML Scraping more match related data\n",
    "\n",
    "Next, we collect more match related data to add to  `df_matches`.\n",
    "For example, the **inducements** and **coach rankings**, as well as match performance stats such as total passing distance `pass`. \n",
    "This information is not available through the API, but each played match has an associated HTML page at https://fumbbl.com/FUMBBL.php?page=match with more info.\n",
    "\n",
    "Since we have to fetch the complete HTML page for each match anyways, I decided to split the proces in two steps:\n",
    "\n",
    "In the first step, the HTML pages for the desired matches are fetched and stored on disk. \n",
    "After some optimization fetching 1K matches takes 10 min. So 6K matches per hour.\n",
    "\n",
    "(Total file size for 154K files is 7GB. These files cannot be stored on Github.)\n",
    "\n",
    "In the second step, the HTML pages are processed with `BeautifulSoup` to extract inducments and coach rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://thehftguy.com/2020/07/28/making-beautifulsoup-parsing-10-times-faster/\n",
    "\n",
    "import lxml\n",
    "import cchardet\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import gzip\n",
    "\n",
    "def do_html_scraping(full_run, begin_match, end_match, verbose = False):\n",
    "\n",
    "    print(\"matches to scrape: \")\n",
    "    n_matches = end_match - begin_match\n",
    "\n",
    "    print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        for i in range(n_matches):\n",
    "            match_id = end_match - i\n",
    "            api_string = \"https://fumbbl.com/FUMBBL.php?page=match&id=\" + str(match_id)\n",
    "\n",
    "            response = requests.get(api_string)\n",
    "\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "            fname_string = dirname + \"/match_\" + str(match_id) + \".html.gz\"\n",
    "            \n",
    "            with gzip.open(fname_string, mode = \"wb\") as f:\n",
    "                f.write(response.text.encode(\"utf-8\"))\n",
    "                f.close()\n",
    "\n",
    "            \n",
    "            if i % 1000 == 0: \n",
    "                if verbose:\n",
    "                # write progress report\n",
    "                    print(i, end='')\n",
    "                    print(\".\", end='')\n",
    "\n",
    "\n",
    "do_html_scraping(full_run = 0, begin_match = 4460944, end_match = 4474450, verbose= True)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the match HTML files\n",
    "\n",
    "I highly recommend [this tutorial](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/) for a great introduction to `BeautifulSoup`.\n",
    "It allows easy access to the information contained within HTML \"div\" tags that partition the web page in different sections. \n",
    "\n",
    "In addition, to clean up the scraped text, I used the **re** Python module (Regular expressions), part of the [Python standard library](https://docs.python.org/3/library/index.html) to extract the actual inducements from the text string that contains them.\n",
    "\n",
    "Now it takes 1.5 min for 1K matches. 40K matches, expect 1h. check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_stuff(full_run, begin_match = None, end_match = None, verbose = False):\n",
    "\n",
    "    if(full_run):\n",
    "        target = 'raw/df_matches_html_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "\n",
    "        print(target)\n",
    "\n",
    "        n_matches = end_match - begin_match\n",
    "    \n",
    "        print(n_matches)\n",
    "        match_id = [] # used CTRL + D to add to selection\n",
    "        team1_inducements = []\n",
    "        team2_inducements = []\n",
    "        coach1_ranking = []\n",
    "        coach2_ranking = []\n",
    "        team1_comp = []\n",
    "        team2_comp = []\n",
    "        team1_pass = []\n",
    "        team2_pass = []\n",
    "        team1_rush = []\n",
    "        team2_rush = []\n",
    "        team1_block = []\n",
    "        team2_block = []\n",
    "        team1_foul = []\n",
    "        team2_foul = []\n",
    "\n",
    "        for i in range(n_matches):\n",
    "            match_id_tmp = end_match - i\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id_tmp)[0:4]\n",
    "\n",
    "            # PM first check gz, if not exist then check for unzipped html\n",
    "            fname_string = dirname + \"/match_\" + str(match_id_tmp) + \".html\"        \n",
    "            fname_string_gz = dirname + \"/match_\" + str(match_id_tmp) + \".html.gz\"        \n",
    "\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                soup = BeautifulSoup(f, 'xml')\n",
    "\n",
    "            if soup.find(\"div\", {\"class\": \"matchrecord\"}) is not None:\n",
    "                # match record is available\n",
    "                inducements = soup.find_all(\"div\", class_=\"inducements\")\n",
    "\n",
    "                pattern = re.compile(r'\\s+Inducements: (.*)\\n')\n",
    "\n",
    "                match = re.match(pattern, inducements[0].get_text())\n",
    "                if match:\n",
    "                    team1_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team1_inducements_tmp = ''\n",
    "\n",
    "                match = re.match(pattern, inducements[1].get_text())\n",
    "                if match:\n",
    "                    team2_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team2_inducements_tmp = ''\n",
    "\n",
    "                coach_info = soup.find_all(\"div\", class_=\"coach\")\n",
    "                # grab the ranking\n",
    "                coach1_ranking_tmp = coach_info[0].get_text()\n",
    "                coach2_ranking_tmp = coach_info[1].get_text()\n",
    "\n",
    "                # match performance stats\n",
    "                div = soup.find_all('div', class_= \"player foot\")\n",
    "                # passing completions\n",
    "                regex = re.compile('.*front comp statbox.*')\n",
    "                team1_comp_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_comp_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()                   \n",
    "                # passing distance in yards\n",
    "                regex = re.compile('.*back pass statbox.*')\n",
    "                team1_pass_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_pass_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # rushes\n",
    "                regex = re.compile('.*back rush statbox.*')\n",
    "                team1_rush_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_rush_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # block\n",
    "                regex = re.compile('.*back block statbox.*')\n",
    "                team1_block_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_block_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # foul\n",
    "                regex = re.compile('.*back foul statbox.*')\n",
    "                team1_foul_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_foul_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "\n",
    "                match_id.append(match_id_tmp) # append for single item, extend for multiple items\n",
    "                team1_inducements.append(team1_inducements_tmp)\n",
    "                team2_inducements.append(team2_inducements_tmp)\n",
    "                coach1_ranking.append(coach1_ranking_tmp)\n",
    "                coach2_ranking.append(coach2_ranking_tmp)\n",
    "                team1_comp.append(team1_comp_tmp)\n",
    "                team2_comp.append(team2_comp_tmp)\n",
    "                team1_pass.append(team1_pass_tmp)\n",
    "                team2_pass.append(team2_pass_tmp)\n",
    "                team1_rush.append(team1_rush_tmp)\n",
    "                team2_rush.append(team2_rush_tmp)\n",
    "                team1_block.append(team1_block_tmp)\n",
    "                team2_block.append(team2_block_tmp)\n",
    "                team1_foul.append(team1_foul_tmp)\n",
    "                team2_foul.append(team2_foul_tmp)\n",
    "            \n",
    "            if i % 1000 == 0: \n",
    "            # write progress report\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "                data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                        coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                        team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                        team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "                df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "                'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "                'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "                'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "                df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "        df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "        'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "        'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "        'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "        df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file    \n",
    "        df_matches_html1 = pd.read_hdf('raw/df_matches_html_20220315_220825.h5') \n",
    "        df_matches_html2 = pd.read_hdf('raw/df_matches_html_20220316_133752.h5') \n",
    "        df_matches_html3 = pd.read_hdf('raw/df_matches_html_20220608_054453.h5')\n",
    "        df_matches_html4 = pd.read_hdf('raw/df_matches_html_20230115_133734.h5')\n",
    "        df_matches_html5 = pd.read_hdf('raw/df_matches_html_20230526_080359.h5')\n",
    "        df_matches_html6 = pd.read_hdf('raw/df_matches_html_20230728_185951.h5')\n",
    "        df_matches_html = pd.concat([df_matches_html1, df_matches_html2, \n",
    "                                     df_matches_html3, df_matches_html4, \n",
    "                                     df_matches_html5, df_matches_html6], ignore_index= True)\n",
    "\n",
    "    return df_matches_html\n",
    "\n",
    "#df_matches_html = do_all_the_stuff(full_run = 1, begin_match = 4460944, end_match = 4474450, verbose= True)\n",
    "df_matches_html = do_all_the_stuff(full_run = 0, verbose= True)\n",
    "\n",
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun -f do_all_the_stuff do_all_the_stuff()\n",
    "# df_matches_html_20230115_133734\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of the profiling: >95% of the time is spend within Beautiful Soup. Switch to xml parser doubled processing speed.\n",
    "No further room for improvements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the performance stats\n",
    "\n",
    "The completions have a few weird values like `4/1`, we drop the slash and the value behind that.\n",
    "`-` is converted to 0 when the string ends directly after the `-` character, i.e. `-` becomes 0 but `-1` becomes -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['team1_comp'] = df_matches_html['team1_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team1_comp'] = pd.to_numeric(df_matches_html['team1_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team2_comp'] = df_matches_html['team2_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team2_comp'] = pd.to_numeric(df_matches_html['team2_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_pass'] = pd.to_numeric(df_matches_html['team1_pass'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_pass'] = pd.to_numeric(df_matches_html['team2_pass'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_rush'] = pd.to_numeric(df_matches_html['team1_rush'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_rush'] = pd.to_numeric(df_matches_html['team2_rush'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_block'] = pd.to_numeric(df_matches_html['team1_block'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_block'] = pd.to_numeric(df_matches_html['team2_block'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_foul'] = pd.to_numeric(df_matches_html['team1_foul'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_foul'] = pd.to_numeric(df_matches_html['team2_foul'].str.replace(r'-$','0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: coach rankings\n",
    "\n",
    "We want to extract the part `CR 149.99` from the scraped coach information field (example `test_string` below). Just as we matches on `Inducements:`, we can match on `CR ` and grab the contents directly after that, stopping when we encounter a whitespace.\n",
    "\n",
    "We first play around a bit and test until we discover the proper Regular Expression to use :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'kingcann Emerging Star CR 149.99 (-0.76)'\n",
    "\n",
    "pattern = re.compile(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "match = re.match(pattern, test_string)\n",
    "\n",
    "if match is not None:\n",
    "    print(match.group(1)) # group(0) is the whole string\n",
    "else:\n",
    "    print(\"match is none\")\n",
    "\n",
    "test_string"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 'm! Now that we have figured it out, we can write the code that extracts the coach rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PM fix CR being 10x as large\n",
    "\n",
    "\n",
    "# Dataprep fix match_id\n",
    "df_matches_html['match_id'] = pd.to_numeric(df_matches_html.match_id) \n",
    "\n",
    "# Dataprep: add the coach rankings as separate cols\n",
    "df_matches_html['coach1_CR'] = df_matches_html['coach1_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "df_matches_html['coach2_CR'] = df_matches_html['coach2_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "df_matches_html['coach1_CR'] = pd.to_numeric(df_matches_html['coach1_CR'])\n",
    "df_matches_html['coach2_CR'] = pd.to_numeric(df_matches_html['coach2_CR'])\n",
    "\n",
    "# abs\n",
    "df_matches_html['CR_diff'] = np.abs(df_matches_html['coach1_CR'] - df_matches_html['coach2_CR'])\n",
    "df_matches_html['CR_diff'] = df_matches_html['CR_diff'].astype(float)\n",
    "\n",
    "# +/-\n",
    "df_matches_html['cr_diff2'] = df_matches_html['coach1_CR'] - df_matches_html['coach2_CR']\n",
    "\n",
    "df_matches_html['cr_diff2_bin'] = pd.cut(df_matches_html['cr_diff2'], bins = [-1*float(\"inf\"), -30, -20, -10, -5, 5, 10, 20, 30, float(\"inf\")], \n",
    " labels=['{-Inf,-30]', '[-30,-20]', '[-20,-10]', '[-10,-5]', '[-5,5]', '[5,10]', '[10,20]', '[20,30]', '[30,Inf]']) \n",
    "\n",
    "df_matches_html['coach1_CR_bin'] = pd.cut(df_matches_html['coach1_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n",
    "\n",
    "df_matches_html['coach2_CR_bin'] = pd.cut(df_matches_html['coach2_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['coach1_CR_bin'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep match inducements for each team\n",
    "\n",
    "The next trick is to use `pandas` `explode()` method (similar to `separate_rows()` in `tidyverse` R) to give each inducement its own row in the dataset.\n",
    "This creates a dataframe (`inducements`) similar to `df_mbt` with each match generating at least two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_inducements = df_matches_html[['match_id', 'team1_inducements']]\n",
    "team2_inducements = df_matches_html[['match_id', 'team2_inducements']]\n",
    "\n",
    "# make column names equal\n",
    "team1_inducements.columns = team2_inducements.columns = ['match_id', 'inducements']\n",
    "team1_inducements['team'] = 'team1'\n",
    "team2_inducements['team'] = 'team2'\n",
    "\n",
    "# row bind the two dataframes\n",
    "inducements = pd.concat([team1_inducements, team2_inducements], ignore_index = True)\n",
    "\n",
    "# convert comma separated string to list\n",
    "inducements['inducements'] = inducements['inducements'].str.split(',')\n",
    "\n",
    "# make each element of the list a separate row\n",
    "inducements = inducements.explode('inducements')\n",
    "\n",
    "# strip leading and trailing whitespaces\n",
    "inducements['inducements'] = inducements['inducements'].str.strip()\n",
    "\n",
    "# create \"star player\" label\n",
    "inducements['star_player'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Star player\"), 'star_player'] = 1\n",
    "\n",
    "# create \"card\" label\n",
    "inducements['special_card'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Card\"), 'special_card'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add match HTML data to df_matches\n",
    "\n",
    "Here we add `df_matches_html` to `df_matches`. This contains each players inducements as a single string, not convenient for analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_matches_html, on='match_id', how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add specific inducement info to df_matches\n",
    "\n",
    "The `inducements` dataframe cannot easily be added to `df_matches`. We can however, extract information from `inducements` at the match level and add this to `df_matches`. Here, I show how to add a 1/0 flag `has_sp` that codes for if a match included any star player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = (inducements\n",
    "            .groupby(\"match_id\")\n",
    "            .agg(has_sp = (\"star_player\", \"max\"))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_matches = pd.merge(df_matches, df_sp, on = \"match_id\", how = \"left\")\n",
    "\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# steps 4-6 add metdata to matches\n",
    "\n",
    "The match JSON is the source for division and division name, these are not present in the tournament endpoint. This is a team property.\n",
    "The tournament JSON Is the source for the group_id (league), these are not present in the match endpoint.\n",
    "The group JSON is the source for the group name and ruleset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "question: is it possible to add league (number / id), and possibly ruleset,  to the match endpoint?\n",
    "Not really, because I don't have it stored there\n",
    "The thing is that teams can be moved across leagues (and therefore rulesets)\n",
    "So you can't know for sure which was being used at the time of the game\n",
    "So even if I take league id from the teams endpoint, it might not be correct for all matches played by that team? Correct.\n",
    "And even for a given league, the base ruleset can change over time (which won't be tracked)\n",
    "And for a given ruleset, there's no guarantee that the settings are the same over time.\n",
    "Much in the same way you can't really look at the current state of a team to see which players were in a game for a given match in the past.\n",
    "You could update nightly for matches really if you wanted to\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 add tournament info\n",
    "\n",
    "create list of all tournament ids.\n",
    "Fetch and store as JSON.\n",
    "Then process and add tournament names to `df_matches`.\n",
    "\n",
    "There are some 4K tournaments in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all tournaments that need to be fetched\n",
    "\n",
    "tournament_ids = df_matches['tournament_id'].values\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "tournament_ids = list(set(tournament_ids))\n",
    "\n",
    "len(tournament_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tournament API call as gzipped JSON object.\n",
    "def update_tourney_jsons(tournament_ids, fullrun = 0):\n",
    "    print(\"total nr of tournaments in the data:\")\n",
    "\n",
    "    n_tourneys = len(tournament_ids)\n",
    "\n",
    "    print(n_tourneys)\n",
    "\n",
    "    if fullrun:\n",
    "        print('fetching tournament data for ', n_tourneys, ' tourneys')\n",
    "\n",
    "        for t in range(n_tourneys):\n",
    "            tournament_id = int(tournament_ids[t])\n",
    "            if t % 100 == 0: \n",
    "                # write progress report\n",
    "                print(t, end='')\n",
    "                print(\".\")\n",
    "\n",
    "            dirname = \"raw/tournament_files/\" + str(tournament_id)[0:2]\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "            fname_string = dirname + \"/tournament_\" + str(tournament_id) + \".json.gz\"\n",
    "\n",
    "            # check if file already exists, else scrape it\n",
    "            try:\n",
    "                f = open(fname_string, mode = \"rb\")\n",
    "\n",
    "            except OSError as e:\n",
    "                # file not present, scrape it         \n",
    "                api_string = \"https://fumbbl.com/api/tournament/get/\" + str(tournament_id)\n",
    "\n",
    "                response = requests.get(api_string)\n",
    "                response = response.json()\n",
    "\n",
    "                with gzip.open(fname_string, mode = \"w\") as f:\n",
    "                    f.write(json.dumps(response).encode('utf-8'))  \n",
    "                    print('x', end = '')\n",
    "                    f.close()\n",
    "                time.sleep(0.3)\n",
    "            else:\n",
    "                # file already present\n",
    "                print(\"o\", end = '')\n",
    "                continue\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"full run is off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_tourney_jsons(tournament_ids, fullrun = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process the tourney json files from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_tourney_stuff(tournament_ids, fullrun):\n",
    "    \n",
    "    n_tourneys = len(tournament_ids)\n",
    "\n",
    "    target = 'raw/df_tourneys_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "\n",
    "    if fullrun:\n",
    "        tournament_id = []\n",
    "        group_id = []\n",
    "        tournament_type = []\n",
    "        tournament_progression = []\n",
    "        tournament_name = []\n",
    "        tournament_start = []\n",
    "        tournament_end = []\n",
    "\n",
    "        print('processing tourney data for ', n_tourneys, ' tournaments')\n",
    "\n",
    "        for t in range(n_tourneys):    \n",
    "            tournament_id_tmp = int(tournament_ids[t])\n",
    "            dirname = \"raw/tournament_files/\" + str(tournament_id_tmp)[0:2]\n",
    "\n",
    "            fname_string_gz = dirname + \"/tournament_\" + str(tournament_id_tmp) + \".json.gz\"        \n",
    "            \n",
    "            # read compressed json file\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                tournament = json.load(f)\n",
    "\n",
    "            if str(tournament) != 'No such tournament.':\n",
    "                # grab fields\n",
    "                tournament_id.append(tournament['id'])\n",
    "                group_id.append(tournament['group'])\n",
    "                tournament_type.append(tournament['type'])\n",
    "                tournament_progression.append(tournament['progression'])\n",
    "                tournament_name.append(tournament['name'])\n",
    "                tournament_start.append(tournament['start'])\n",
    "                tournament_end.append(tournament['end'])   \n",
    "\n",
    "            if t % 500 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(t, end='')\n",
    "                print(\".\", end='')\n",
    "        # create list of tuples\n",
    "        data = zip(tournament_id, group_id, tournament_type, tournament_progression, tournament_name, tournament_start, tournament_end)\n",
    "        # create dataframe from list\n",
    "        df_tourneys = pd.DataFrame(data, columns=['tournament_id', 'group_id', 'tournament_type', 'tournament_progression', \n",
    "        'tournament_name', 'tournament_start', 'tournament_end'])\n",
    "        df_tourneys.to_hdf(target, key='df_tourneys', mode='w')   \n",
    "\n",
    "\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        df_tourneys = pd.read_hdf('raw/df_tourneys_20230528_143734.h5')\n",
    "\n",
    "    df_tourneys['tournament_id'] = pd.to_numeric(df_tourneys.tournament_id) \n",
    "    df_tourneys['group_id'] = pd.to_numeric(df_tourneys.group_id) \n",
    "    df_tourneys['tournament_type'] = pd.to_numeric(df_tourneys.tournament_type) \n",
    "\n",
    "    return df_tourneys\n",
    "\n",
    "df_tourneys = do_all_the_tourney_stuff(tournament_ids, fullrun = 1)\n",
    "\n",
    "df_tourneys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys.sort_values(by='tournament_id', inplace=True, ascending=False)\n",
    "df_tourneys = df_tourneys.reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 Fetch groups (for groupname and ruleset)\n",
    "\n",
    "Ok, let me start from the beginning with terminology ðŸ™‚\n",
    "A ruleset is basically the root of the whole thing at this point. This is where rosters and rule options are defined.\n",
    "A division is an environment for \"open play\", and used to be the root of the data model before rulesets were a thing.\n",
    "A league is a special kind of group, which is defined by it having a ruleset configured.\n",
    "A group is a \"meta group\" and currently acts mostly as a parent for tournaments\n",
    "A tournament is the actual scheduling systems put into play.\n",
    "\n",
    "A \"League\" is a special kind of \"Group\"?\n",
    "\\\n",
    "Yes, they are technically the same, but the league is configured with a custom ruleset (and therefore doesn't use the division default ruleset)\n",
    "The distinction between a group and a league is very narrow, but has a huge effect.\n",
    "In a sense the \"league\" is equivalent to a custom division.\n",
    "https://discord.com/channels/254387387260469258/739746315449139240/1080863062342504478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ids = df_tourneys['group_id'].values\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "group_ids = list(set(group_ids))\n",
    "\n",
    "len(group_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save group API call as gzipped JSON object.\n",
    "def update_group_jsons(group_ids, fullrun = 0):\n",
    "    print(\"total nr of groups in the data:\")\n",
    "\n",
    "    n_groups = len(group_ids)\n",
    "\n",
    "    print(n_groups)\n",
    "\n",
    "    if fullrun:\n",
    "        print('fetching group data for ', n_groups, ' groups')\n",
    "\n",
    "        for t in range(n_groups):\n",
    "            group_id = int(group_ids[t])\n",
    "            if t % 100 == 0: \n",
    "                # write progress report\n",
    "                print(t, end='')\n",
    "                print(\".\")\n",
    "\n",
    "            dirname = \"raw/group_files/\" + str(group_id)[0:2]\n",
    "            if not os.path.exists(dirname):\n",
    "                os.makedirs(dirname)\n",
    "\n",
    "            fname_string = dirname + \"/group_\" + str(group_id) + \".json.gz\"\n",
    "\n",
    "            # check if file already exists, else scrape it\n",
    "            try:\n",
    "                f = open(fname_string, mode = \"rb\")\n",
    "\n",
    "            except OSError as e:\n",
    "                # file not present, scrape it         \n",
    "                api_string = \"https://fumbbl.com/api/group/get/\" + str(group_id)\n",
    "\n",
    "                response = requests.get(api_string)\n",
    "                response = response.json()\n",
    "\n",
    "                with gzip.open(fname_string, mode = \"w\") as f:\n",
    "                    f.write(json.dumps(response).encode('utf-8'))  \n",
    "                    print('x', end = '')\n",
    "                    f.close()\n",
    "                time.sleep(0.3)\n",
    "            else:\n",
    "                # file already present\n",
    "                print(\"o\", end = '')\n",
    "                continue\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"full run is off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_group_jsons(group_ids, fullrun = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group_jsons(group_ids, fullrun):\n",
    "\n",
    "    n_groups = len(group_ids)\n",
    "\n",
    "    target = 'raw/df_groups_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "\n",
    "    if fullrun:\n",
    "        group_id = []\n",
    "        group_name = []\n",
    "        ruleset = []\n",
    "\n",
    "        print('processing group data for ', n_groups, ' groups')\n",
    "\n",
    "        for t in range(n_groups):    \n",
    "            group_id_tmp = int(group_ids[t])\n",
    "            dirname = \"raw/group_files/\" + str(group_id_tmp)[0:2]\n",
    "\n",
    "            fname_string_gz = dirname + \"/group_\" + str(group_id_tmp) + \".json.gz\"        \n",
    "            \n",
    "            # read compressed json file\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                group = json.load(f)\n",
    "\n",
    "            if str(group) != 'No such group.':\n",
    "                # grab fields\n",
    "                group_id.append(group['id'])\n",
    "                group_name.append(group['name'])\n",
    "                ruleset.append(group['ruleset'])\n",
    "\n",
    "            if t % 500 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(t, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "        data = zip(group_id, group_name, ruleset)\n",
    "        df_groups = pd.DataFrame(data, columns=['group_id', \n",
    "        'group_name', 'ruleset'])\n",
    "        df_groups.to_hdf(target, key='df_groups', mode='w')   \n",
    "\n",
    "\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        df_groups = pd.read_hdf('raw/df_groups_20230528_143736.h5')\n",
    "\n",
    "    df_groups['group_id'] = pd.to_numeric(df_groups.group_id) \n",
    "    df_groups['ruleset'] = pd.to_numeric(df_groups.ruleset) \n",
    "\n",
    "    return df_groups\n",
    "\n",
    "df_groups = process_group_jsons(group_ids, fullrun = 1)\n",
    "df_groups = df_groups.rename(columns={\"ruleset\": \"current_ruleset\"})\n",
    "df_groups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tourneys2 = pd.merge(df_tourneys, df_groups, on = 'group_id', how = 'left') # lost a few\n",
    "\n",
    "df_tourneys2.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add tourney, group and division data to match data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.iloc[[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_tourneys2, on='tournament_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.query(\"match_id == 4421729\")\n",
    "df_matches.shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6  Add ruleset_version and division_name to df_matches\n",
    "\n",
    "Divisions are very important in FUMBBL, every team belongs to a Division.\n",
    "\n",
    "PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310? this has a few changes (tier, gold, crossleague)\n",
    "* Do we also see this in the XML API\n",
    "\n",
    "Lets have look at the various divisions and leagues, and which rulesets are used.\n",
    "There are a lot of small leagues being played on FUMBBL, they account for maybe X% of all the matches.\n",
    "\n",
    "We only look at divisions and leagues with a sufficient volume of matches, or otherwise we do not have sufficient statistics for each race.\n",
    "\n",
    "So I aggregated the data by division, league and ruleset, and filtered on at least 150 different teams that have played at least once last year.\n",
    "Apart from the main \"Divisions\" that are part of FUMBBL, there were a few user-run leagues present in this table, so I looked up their names on FUMBBL and what ruleset is used (BB2016, BB2020 or some other variant). This information (contained in an xlsx) is added to the dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = (df_matches\n",
    ".groupby([ 'division_id', 'division_name', 'scheduler' , 'year', 'group_id', 'group_name',  'tournament_id' , 'tournament_name'], dropna=False)\n",
    ".agg(\n",
    "    n_games = ('match_id', 'count')\n",
    ")\n",
    ".reset_index()\n",
    ".sort_values(\"n_games\", ascending=False)\n",
    ")\n",
    "\n",
    "#res.to_excel('group_counts.xlsx')\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conclusion, major divisions are not found. Those (teams /) matches do not have a tournament id.\n",
    "But clearly distinguished.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all prepped datasets as HDF5 and CSV files\n",
    "\n",
    "HD5 files can be read by both Python and [R](https://cran.r-project.org/web/packages/hdf5r/index.html) and preserve column data types.\n",
    "\n",
    "**Update** the HDF5 schema used by pandas is highly specific to pandas, it is not designed for external use. An R code snippet is available but does not work with table format. For now using the CSVs for R is advised\n",
    "\n",
    "CSV files are the lingua franca across all data analysis software.\n",
    "\n",
    "A dataset release consists of three datasets:\n",
    "* A list of matches, identified by match_id\n",
    "* A list of matches by team, identified by match_id and team_id\n",
    "* A list of inducements by match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to locate variables that cannot be serialized by hdf5\n",
    "#df_matches.loc[:, :'week_date'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50000  #chunk row size\n",
    "list_df = [df_matches[i:i+n] for i in range(0,len(df_matches),n)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_matches'\n",
    "\n",
    "for i in range(len(list_df)):\n",
    "    print(list_df[i].shape)\n",
    "    if i == 0:\n",
    "        list_df[i].to_hdf(target + '.h5', key='df_matches', mode='w', format = 'table',  complevel = 9, min_itemsize=750)\n",
    "    else:\n",
    "        list_df[i].to_hdf(target + '.h5', key='df_matches', append = True, mode='r+', format = 'table', complevel = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_matches'\n",
    "\n",
    "df_matches.to_csv(target + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/inducements'\n",
    "inducements.to_hdf(target + '.h5', key='inducements', mode='w', format = 't',  complevel = 9)\n",
    "inducements.to_csv(target + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_tourneys'\n",
    "\n",
    "df_tourneys.to_hdf(target + '.h5', key='df_tourneys', mode='w', format = 't',  complevel = 9)\n",
    "df_tourneys.to_csv(target + '.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a license for the public dataset\n",
    "\n",
    "An important part of making data publicly available is being explicit about what is allowed if people want to use the dataset.\n",
    "However, before we do so, we have to check if **we** are actually allowed to publish the data. This is explained nicely [in a blogpost by Elizabeth Wickes](https://datacarpentry.org/blog/2016/06/data-licensing).\n",
    "\n",
    "Since our data comes from the **FUMBBL.com** website, we check the [**Privacy policy**](https://fumbbl.com/p/privacy) where all users, including myself have agreed on when signing up. It contains this part which is specific to the unauthenticated API, which we use to fetch the data, as well as additional public match data, such as which inducements are used in a match, and the Coach rankings of the playing coaches that were current when the match was played.\n",
    "\n",
    "```\n",
    "Content you provide through the website\n",
    "All the information you provide through the website is processed by FUMBBL. This includes things such as forum posts, private message posts, blog entries, team and player names and biographies and news comments. Data provided this way is visible by other people on the website and in most cases public even to individuals without accounts (not including private messages), and as such are considered of public interest. If direct personal information is posted in public view, you can contact moderators to resolve this. Match records are also considered content in this context, and is also considered of public interest. This data is collected as the primary purpose of the website and it is of course entirely up to you how much of this is provided to FUMBBL. \n",
    "\n",
    "Third party sharing\n",
    "Some of the public data is available through a public (*i.e. unauthenticated*) API, which shares some of the information provided by FUMBBL users in a way suitable for third-party websites and services to process.\n",
    "\n",
    "The data available through the unauthenticated API is considered non-personal as it only reflects information that is public by its nature on the website. The authenticated API will only show information connected to the authenticated account.\n",
    "```\n",
    "\n",
    "I conclude that since the match data is already considered public content, there is no harm in collecting this public data in a structured dataset and placing this data in a public repository. I also verified this with Christer, the site owner. \n",
    "\n",
    "\n",
    "The final step is then to decide what others are allowed to do with this data. In practice, this means choosing a license under which to release the dataset. I decided to choose a CC0 license: this places the data in the public domain, and people can use the dataset as they wish. Citing or mentioning the source of the data would still be appreciated of course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of possible future improvements\n",
    "\n",
    "* Scraping the players (only most recent version, so no player development history)\n",
    "* Scraping the rulesets (for example to identify resurrection tournaments where players choose skills and use tiers)\n",
    "\n",
    "* catch exception: **PM we cannot deal yet with the situation HTTPSConnectionPool(host='fumbbl.com', port=443): Max retries exceeded with url: /api/match/get/4221820 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4acff12be0>: Failed to establish a new connection: [Errno 110] Connection timed out',))**\n",
    "\n",
    "* cr_bin variable is gone?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50276fd1884268afe39607052f22ef19b84d915691d702a5c7e9a67a09867105"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('requests_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
