{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of a public dataset of Blood Bowl matches played on FUMBBL.com\n",
    "\n",
    "This blogpost is about **Blood Bowl**, a boardgame I started playing last year. The goal of this blog post is to use Python API and HTML scraping to fetch online Blood Bowl match outcome data, and to create a structured dataset ready for analysis and visualization. In a [separate blog post](link/to/blog) I'll showcase some analyses on this dataset (also known as [nuffalytics](www.nufflytics.com) in reference to Nuffle, the god of Blood Bowl). The dataset was primarily constructed to analyze rule changes that were introduced on FUMBBL.com last year, and how these affected the relative strengths of the different teams coaches can pick to field against each other.\n",
    "\n",
    "In writing this blog post, I took inspiration from a relatively new development in Open Science, that of the **Data paper** or **Data publication** (Chavan & Penev, 2011). A data paper is a (ideally peer reviewed) publication of a dataset as a stand alone research output. A Data paper can be thought of similar to the Section “Methods” of a traditional research article, though with greater detail. The paper describes the contents of the dataset, the data acquisition process,  and includes a discussion of the motivation and considerations regarding experimental design (if applicable). Data papers do not provide any analysis nor results / conclusions. The dataset itself should be online available at a data repository such as **Zenodo**, **Figshare** or **Dryad**. The datasets described below are available (for now) in a separate [Github Repository](https://github.com/gsverhoeven/fumbbl_datasets).\n",
    "\n",
    "To give an impression of how this works out in practice, here are two examples: The first data paper, [*A public data set of spatio-temporal match events in soccer competitions*](https://www.nature.com/articles/s41597-019-0247-7), was published in \"Scientific data\" with the dataset hosted at [Figshare](https://doi.org/10.6084/m9.figshare.c.4415000.v5). The second is collected from Twitter: [*A large-scale COVID-19 Twitter chatter dataset for open scientific research -- an international collaboration*](https://arxiv.org/abs/2004.03688), with the open dataset published on [Zenodo](https://doi.org/10.5281/zenodo.5775023).\n",
    "\n",
    "*Chavan, V., and Penev, L. (2011). The data paper: a mechanism to incentivize data publishing in biodiversity science. BMC Bioinformatics 12(Suppl. 15):S2. doi:10.1186/1471-2105-12-S15-S2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software needed to reproduce this blog post\n",
    "\n",
    "This blogpost is written as a Jupyter notebook containing Python code, and is fully reproducible. The idea is to make Blood Bowl data analysis accessible to others. Using open source tooling reduces the barriers for others to build on other people’s work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "from isoweek import Week\n",
    "\n",
    "import requests # API library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blood Bowl online: FUMBBL \n",
    "\n",
    "The **FUMBBL** website (https://fumbbl.com) is one big pile of data. From coach pages, with their teams, to team rosters, with players, and match histories. It's all there.\n",
    "\n",
    "To obtain **FUMBBL** data, we need to fetch it match by match, team by team. To do so, the site creator Christer Kaivo-oja, from Sweden, has made an API that allows us to easily fetch data. What follows is a short demonstration how the API works, before we fetch the **FUMBBL** match and team data of the last 12 months.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behold, the power of Requests\n",
    "We use the [Python **Requests** library](https://docs.python-requests.org/en/latest/) to make the API call over HTTPS and obtain the response from the FUMBLL server. The response is in the JSON format, a [light-weight data-interchange format](https://www.json.org/json-en.html) which is both easy to read and write for humans, and easy to parse and generate by computers. So this makes it a natural choice for an API.\n",
    "\n",
    "Here is an example of what is available at the coach level. The full documentation of the API can be found at (https://fumbbl.com/apidoc/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://fumbbl.com/api/coach/teams/gsverhoeven'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, timeout = 10)\n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"Timeout: Maybe set up for a retry, or continue in a retry loop\")\n",
    "except requests.exceptions.TooManyRedirects:\n",
    "    print(\"TooManyRedirects: Tell the user their URL was bad and try a different one\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # catastrophic error. bail.\n",
    "    raise SystemExit(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the complete JSON object {}\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing JSON data\n",
    "\n",
    "Let's have a closer look at the JSON data structure here.\n",
    "We have a list of key-value pairs. \n",
    "Using brackets `[]` we can choose keys and retrieve their values.\n",
    "Some keys contain simple values, such as `name`, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but some return as value a new list of key-value pairs, such as `teams`.\n",
    "Actually this is a \"list of \"lists of key-value pairs\", since we have a separate list for each team.\n",
    "Even the list of a single team contains new structure, for example under the key `raceLogos`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['teams'][2]['raceLogos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.json()['teams'][2]['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What data do we need? And in what shape?\n",
    "\n",
    "Now we know how the data comes in, we need to think about which variables we want, and how to structure them.\n",
    "The most straightforward level to analyze race strength is to look at **match outcomes**.\n",
    "At its core, the data consists of matches played by teams, commanded by coaches.\n",
    "Furthermore, we expect race strength to change over time, as new strategies are discovered by the players, or new rules get introduced. So the time dimension is important as well.\n",
    "\n",
    "So, let's go with a flat data frame with **rows for each match**, and columns for the various variables associated with each match.\n",
    "These would include:\n",
    "\n",
    "* Coach ids\n",
    "* Team races\n",
    "* Team ids\n",
    "* Date of the match\n",
    "* Outcome (Touchdowns of both teams)\n",
    "\n",
    "With this basic structure, we can add as many match related variables in the future, keeping the basic structure (each row is a match) unchanged.\n",
    "\n",
    "So lets get the match data!\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: API scraping the match data: df_matches\n",
    "\n",
    "So we are mostly intested in the current ruleset, this is `BB2020`. This ruleset became available in **FUMBBL** at september 1st 2021, and two months later, some 5000 games have been played. We also want to compare with the previous ruleset, where we have much more data available. How far do we go back? \n",
    "Lets try for roughly 12 months of `BB2016` ruleset matches, and a few months of `BB2020` matches.\n",
    "\n",
    "The easiest way to collect match data over a particular period of time is to just loop over `match_id`. The most recent match at the time of writing was 4.347.800, and since rougly 100.000 matches are played each year, we can fiddle about and we find match `4216258` played on august 1st, 2020.  So that means we need to collect some 130K matches. \n",
    "\n",
    "**VERY IMPORTANT: We do not want to overload the **FUMBBL** server, so we make only three API requests per second. In this way, the server load is hardly affected and it can continue functioning properly for all the Blood Bowl coaches playing their daily games!**\n",
    "\n",
    "To collect 130K matches, we will need 110000*0.333/3600 = 15 hours.\n",
    "\n",
    "PM split fetching and processing of matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# estimated hours fetching data\n",
    "[(4369933-4216257), (4369933-4216257)*0.333/3600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4221820+50000+50000+50000-1320+43\n",
    "#4370543\n",
    "# 4386470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run write_json_file.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.DataFrame(columns=['match_id', 'replay_id', 'tournament_id', 'match_date', 'match_time',  'match_conceded',\n",
    "    'team1_id', 'team1_coach_id', 'team1_roster_id', 'team1_race_name', 'team1_value', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'team2_id', 'team2_coach_id', 'team2_roster_id', 'team2_race_name', 'team2_value', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'team1_score', 'team2_score'])\n",
    "\n",
    "target = 'raw/df_matches_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "print(target)\n",
    "\n",
    "end_match = 4429319 \n",
    "begin_match = 4386470\n",
    "n_matches = end_match - begin_match\n",
    "full_run = 0\n",
    "print(\"matches to grab:\")\n",
    "print(n_matches)\n",
    "\n",
    "if(full_run):\n",
    "    for i in range(n_matches):\n",
    "        match_id = end_match - i\n",
    "        api_string = \"https://fumbbl.com/api/match/get/\" + str(match_id)\n",
    "\n",
    "        match = requests.get(api_string)\n",
    "        match = match.json()\n",
    "\n",
    "        dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/match_\" + str(match_id) + \".json\"\n",
    "\n",
    "        write_json_file(match, fname_string)\n",
    "\n",
    "        if match: # fix for matches that do not exist\n",
    "            match_id = match['id']\n",
    "            replay_id = match['replayId']\n",
    "            tournament_id = match['tournamentId'] # key to tournament table\n",
    "            match_date = match['date']\n",
    "            match_time = match['time']\n",
    "            match_conceded = match['conceded']\n",
    "            team1_id = match['team1']['id']\n",
    "            team2_id = match['team2']['id']\n",
    "            # touchdowns\n",
    "            team1_score = match['team1']['score']\n",
    "            team2_score = match['team2']['score']  \n",
    "            # casualties\n",
    "            team1_cas_bh = match['team1']['casualties']['bh']\n",
    "            team1_cas_si = match['team1']['casualties']['si']\n",
    "            team1_cas_rip = match['team1']['casualties']['rip']\n",
    "            team2_cas_bh = match['team2']['casualties']['bh']\n",
    "            team2_cas_si = match['team2']['casualties']['si']\n",
    "            team2_cas_rip = match['team2']['casualties']['rip']\n",
    "            # other\n",
    "            team1_roster_id = match['team1']['roster']['id']\n",
    "            team2_roster_id = match['team2']['roster']['id']            \n",
    "            team1_coach_id = match['team1']['coach']['id']\n",
    "            team2_coach_id = match['team2']['coach']['id']\n",
    "            team1_race_name = match['team1']['roster']['name'] \n",
    "            team2_race_name = match['team2']['roster']['name'] \n",
    "            team1_value = match['team1']['teamValue']\n",
    "            team2_value = match['team2']['teamValue']\n",
    "            #print(match_id)     \n",
    "            df_matches.loc[i] = [match_id, replay_id, tournament_id, match_date, match_time, match_conceded,\n",
    "                team1_id, team1_coach_id, team1_roster_id, team1_race_name, team1_value, team1_cas_bh, team1_cas_si, team1_cas_rip, \n",
    "                team2_id, team2_coach_id, team2_roster_id, team2_race_name, team2_value, team2_cas_bh, team2_cas_si, team2_cas_rip, \n",
    "                team1_score, team2_score]\n",
    "        else:\n",
    "            # empty data for this match, create empty row\n",
    "            match_id = int(end_match - i)\n",
    "            df_matches.loc[i] = np.repeat([np.NaN], 24, axis=0)\n",
    "            df_matches.loc[i]['match_id'] = int(match_id)\n",
    "        if i % 100 == 0: \n",
    "            # write tmp data as hdf5 file\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "            df_matches.to_hdf(target, key='df_matches', mode='w')\n",
    "\n",
    "    # write data as hdf5 file\n",
    "    df_matches.to_hdf(target, key='df_matches', mode='w')\n",
    "else:\n",
    "    # read from hdf5 file\n",
    "    df_matches1 = pd.read_hdf('raw/df_matches_20220310_155600.h5') # 4216259 - 4221258\n",
    "    df_matches2 = pd.read_hdf('raw/df_matches_20220316_180506.h5') # 4221259 - 4221820\n",
    "    df_matches3 = pd.read_hdf('raw/df_matches_20220311_084424.h5') # 4221821 - 4271820\n",
    "    df_matches4 = pd.read_hdf('raw/df_matches_20220311_231408.h5') # 4271821 - 4321820\n",
    "    df_matches5 = pd.read_hdf('raw/df_matches_20220312_083221.h5') # 4321821 - 4370543\n",
    "    df_matches6 = pd.read_hdf('raw/df_matches_20220606_225206.h5') # 4374338 - 4386470\n",
    "    df_matches7 = pd.read_hdf('raw/df_matches_20220607_060907.h5') # 4370544 - 4374337\n",
    "    df_matches8 = pd.read_hdf('raw/df_matches_20230108_205542.h5') # 4429319 - 4386470\n",
    "    df_matches = pd.concat([df_matches1, df_matches2, df_matches3, \n",
    "        df_matches4, df_matches5, df_matches6, df_matches7, df_matches8], ignore_index=True)\n",
    "\n",
    "df_matches.shape\n",
    "df_matches = df_matches.sort_values(by=['match_id']).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.query('match_id == 4429319')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep: fixing the datatypes, creating derived variables\n",
    "\n",
    "Since we manually filled the `pandas` DataFrame, most of the columns are now of `object` datatype.\n",
    "We need to change this to be able to work properly with the data, as well as store it properly.\n",
    "Here I convert each column manually, however I later found out about `DataFrame.infer_objects()`, that can detect the proper dtype automatically.\n",
    "This I will try next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert object dtype columns to proper pandas dtypes datetime and numeric\n",
    "df_matches['match_date'] = pd.to_datetime(df_matches.match_date) # Datetime object\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) \n",
    "df_matches['replay_id'] = pd.to_numeric(df_matches.replay_id) \n",
    "df_matches['tournament_id'] = pd.to_numeric(df_matches.tournament_id) \n",
    "df_matches['team1_id'] = pd.to_numeric(df_matches.team1_id) \n",
    "df_matches['team1_coach_id'] = pd.to_numeric(df_matches.team1_coach_id) \n",
    "df_matches['team1_roster_id'] = pd.to_numeric(df_matches.team1_roster_id) \n",
    "df_matches['team2_id'] = pd.to_numeric(df_matches.team2_id) \n",
    "df_matches['team2_coach_id'] = pd.to_numeric(df_matches.team2_coach_id) \n",
    "df_matches['team2_roster_id'] = pd.to_numeric(df_matches.team2_roster_id) \n",
    "df_matches['team1_score'] = pd.to_numeric(df_matches.team1_score) \n",
    "df_matches['team2_score'] = pd.to_numeric(df_matches.team2_score) \n",
    "\n",
    "# calculate match score difference\n",
    "df_matches['team1_win'] = np.sign(df_matches['team1_score'] - df_matches['team2_score'])\n",
    "df_matches['team2_win'] = np.sign(df_matches['team2_score'] - df_matches['team1_score'])\n",
    "\n",
    "# mirror match\n",
    "df_matches['mirror_match'] = 0\n",
    "df_matches.loc[df_matches['team1_race_name'] == df_matches['team2_race_name'], 'mirror_match'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['team1_cas_bh'] = pd.to_numeric(df_matches.team1_cas_bh) \n",
    "df_matches['team1_cas_si'] = pd.to_numeric(df_matches.team1_cas_si) \n",
    "df_matches['team1_cas_rip'] = pd.to_numeric(df_matches.team1_cas_rip) \n",
    "# add total CAS\n",
    "df_matches['team1_cas'] = df_matches['team1_cas_bh'] + df_matches['team1_cas_si'] + df_matches['team1_cas_rip']\n",
    "\n",
    "\n",
    "df_matches['team2_cas_bh'] = pd.to_numeric(df_matches.team2_cas_bh) \n",
    "df_matches['team2_cas_si'] = pd.to_numeric(df_matches.team2_cas_si) \n",
    "df_matches['team2_cas_rip'] = pd.to_numeric(df_matches.team2_cas_rip) \n",
    "# add total CAS\n",
    "df_matches['team2_cas'] = df_matches['team2_cas_bh'] + df_matches['team2_cas_si'] + df_matches['team2_cas_rip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirror matches\n",
    "df_matches.query('mirror_match == 1').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: transforming the team values\n",
    "\n",
    "In Blood Bowl, teams can develop themselves over the course of multiple matches. The winnings of each match can be spend on buying new, stronger players, or replace the players that ended up getting injured or even killed. In addition, players receive so-called *star points* for important events, such as scoring, or inflicting a casualty on the opponent. Therefore, a balancing mechanism is needed when a newly created \"rookie\" team is facing a highly developed opposing team with lots of extra skills and strong players. \n",
    "\n",
    "Blood Bowl solves this by calculating for both teams their **Current team value**.\n",
    "The **Team value difference** for a match determines the amount of gold that the weaker team can use to buy so-called **inducements**.\n",
    "These inducements are temporary, and can consists of a famous \"star player\" who joins the team just for this match. Another popular option is to hire a wizard that can be used to turn one of the opposing players into a frog.\n",
    "\n",
    "It is well known that the win rates of the teams depend on how developed a team is. For example, Amazons are thought to be strongest at low team value, as they already start out with lots of *block* and *dodge* skills, whereas a Chaos team start out with almost no skills.\n",
    "So if we compare win rates, we would like take into account the current team value. \n",
    "Now as this can differ between the two teams in a match up, I reasoned that the highest team value is most informative about the average strength level of both teams, because of the inducement mechanism described above. (In the next step, we will add information on inducements)\n",
    "\n",
    "In the dataset, we have for each match the current team values of both teams as a text string. \n",
    "We transform the text string `1100k` into an integer number `1100`, so that we can calculated the difference as `tv_diff`, and pick for each match the maximum team value and store it as `tv_match`. Finally, we create a team value bin `tv_bin` to be able to compare win rates for binned groups of matches where races have comparable team strength / team development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert team value 1100k to 1100 integer and and above / below median (= low / high TV)\n",
    "df_matches['team1_value'] = df_matches['team1_value'].str.replace('k$', '')\n",
    "df_matches['team1_value'] = df_matches['team1_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['team2_value'] = df_matches['team2_value'].str.replace('k$', '')\n",
    "df_matches['team2_value'] = df_matches['team2_value'].fillna(0).astype(np.int64)\n",
    "\n",
    "df_matches['tv_diff'] = np.abs(df_matches['team2_value'] - df_matches['team1_value'])\n",
    "df_matches['tv_diff2'] = df_matches['team2_value'] - df_matches['team1_value']\n",
    "\n",
    "df_matches['tv_match'] = df_matches[[\"team1_value\", \"team2_value\"]].max(axis=1)\n",
    "\n",
    "df_matches['tv_bin'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1250,1550, 1850, float(\"inf\")], \n",
    "    labels=['< 950K', '1.1M', '1.4M', '1.7M', '> 1850K']\n",
    ")\n",
    "\n",
    "df_matches['tv_bin2'] = pd.cut(df_matches['tv_match'], \n",
    "    bins = [0, 950, 1050, 1150, 1250, 1350, 1450, 1550, float(\"inf\")], \n",
    "    labels=['< 950K', '1.0M', '1.1M', '1.2M',  '1.3M', '1.4M', '1.5M', '> 1550K']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping empty matches\n",
    "\n",
    "Some match_id's do not have match information attached to them, presumably these matches were not played or some real life event interfered. These match_ids are dropped from the dataset to get rid of the NAs in all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_matches)\n",
    "df_matches = df_matches.dropna(subset=['match_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: getting the dates right\n",
    "\n",
    "To see time trends, its useful to aggregate the data by week. For this we add `week_number` for each date, and from this week number, we convert back to a date to get a `week_date`. This last part is useful for plotting with `plotnine`, as this treats dates in a special way\n",
    "We use the ISO definition of week, this has some unexpected behavior near the beginning / end of each year. \n",
    "\n",
    "The data starts in week 36 (september) of 2020, and stops halfway march 2022.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches['week_number'] = df_matches['match_date'].dt.isocalendar().week\n",
    "\n",
    "# cannot serialize numpy int OR Int64 when writing HDF5 file, so we go for plain int as all NAs are gone now\n",
    "df_matches['week_number'] = df_matches['week_number'].fillna(0).astype(int)\n",
    "\n",
    "# add year based on match ISO week\n",
    "df_matches['year'] = df_matches['match_date'].dt.isocalendar().year.astype(int)\n",
    "\n",
    "df_matches['week_year'] = df_matches['year'].astype(str) + '-' + df_matches['week_number'].astype(str)\n",
    "\n",
    "# use a lambda function since isoweek.Week is not vectorized \n",
    "df_matches['week_date'] = pd.to_datetime(df_matches.apply(lambda row : Week(int(row[\"year\"]),int(row[\"week_number\"])).monday(),axis=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: HTML Scraping more match related data\n",
    "\n",
    "Next, we collect more match related data to add to  `df_matches`.\n",
    "For example, the **inducements** and **coach rankings**, as well as match performance stats such as total passing distance `pass`. \n",
    "This information is not available through the API, but each played match has an associated HTML page at https://fumbbl.com/FUMBBL.php?page=match with more info.\n",
    "\n",
    "Since we have to fetch the complete HTML page for each match anyways, I decided to split the proces in two steps:\n",
    "\n",
    "In the first step, the HTML pages for the desired matches are fetched and stored on disk. \n",
    "This takes 10 hours for 50K matches. \n",
    "\n",
    "Now it seems 1K matches is 10 min. So 6K matches per hour.\n",
    "\n",
    "(Total file size for 154K files is 7GB. These files cannot be stored on Github.)\n",
    "\n",
    "In the second step, the HTML pages are processed with `BeautifulSoup` to extract inducments and coach rankings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://thehftguy.com/2020/07/28/making-beautifulsoup-parsing-10-times-faster/\n",
    "\n",
    "import lxml\n",
    "import cchardet\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import gzip\n",
    "\n",
    "end_match = 4429319 \n",
    "begin_match = 4386470\n",
    "\n",
    "print(\"matches to scrape: \")\n",
    "n_matches = end_match - begin_match\n",
    "\n",
    "full_run = 0\n",
    "\n",
    "print(n_matches)\n",
    "\n",
    "if(full_run):\n",
    "    for i in range(n_matches):\n",
    "        match_id = end_match - i\n",
    "        api_string = \"https://fumbbl.com/FUMBBL.php?page=match&id=\" + str(match_id)\n",
    "\n",
    "        response = requests.get(api_string)\n",
    "\n",
    "        dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/match_\" + str(match_id) + \".html.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"wb\") as f:\n",
    "            f.write(response.text.encode(\"utf-8\"))\n",
    "            f.close()\n",
    "\n",
    "        if i % 1000 == 0: \n",
    "            # write progress report\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b: extract player ids from match data (NOT ACTIVE RIGHT NOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we write a function that, given a match_id, reads the contents in BeautifulSoup and extracts the stuff we need.\n",
    "It returns them as separate lists, later to be combined in a pd df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_ids_from_single_file(match_id):\n",
    "    \n",
    "    dirname = \"raw/match_html_files/\" + str(match_id)[0:4]\n",
    "    fname_string = dirname + \"/match_\" + str(match_id) + \".html\"  \n",
    "\n",
    "    with open(fname_string, mode = \"r\") as f:\n",
    "        # https://stackoverflow.com/questions/30277109/beautifulsoup-takes-forever-can-this-be-done-faster\n",
    "        soup = BeautifulSoup(f, 'xml')\n",
    "\n",
    "    if soup.find(\"div\", {\"class\": \"matchrecord\"}) is not None:\n",
    "        # match record is available\n",
    "        player_id = []\n",
    "        player_number = []\n",
    "        player_name = []\n",
    "\n",
    "        players = soup.find_all(\"div\", class_=\"player\")\n",
    "\n",
    "        for p in range(len(players)):\n",
    "            div = players[p].find(\"div\", class_= \"name\")\n",
    "            # https://stackoverflow.com/questions/55442727/remove-unicode-xa0-from-pandas-column\n",
    "            div_number = players[p].find(\"div\", class_= \"number\")\n",
    "            if div_number is not None:\n",
    "                div_number = div_number.get_text().strip()\n",
    "                if div_number is not '':\n",
    "                    player_number.append(div_number)\n",
    "\n",
    "            for a in div.find_all('a'):\n",
    "                # url to scrape\n",
    "                #player_number = p\n",
    "                player_url = a.get('href') #for getting link\n",
    "                player_id.append(player_url.split('=', 1)[1])\n",
    "                # player name\n",
    "                player_name.append(a.text) #for getting text between the link\n",
    "        return [match_id] * len(player_id), player_id, player_number, player_name\n",
    "    else:\n",
    "        # NOT SMART, this will fuck up the data types (PyTables pickle performance warning)\n",
    "        return [match_id], [-1], [None], [None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Profile the workhorse function line by line. Virtually all of the time is now spend in BeautifulSoup, with 70% in the BS parser, and 30% in the find calls that filter out the stuff we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "%lprun -f extract_player_ids_from_single_file extract_player_ids_from_single_file(4386470)\n",
    "\n",
    "# 31% of the time is parsing the HTML!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_player_ids_from_single_file(4353200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the match id - player id list from the match HTML files\n",
    "\n",
    "This gives us for each match the participating player ids.\n",
    "Each match contains on average 25 players.\n",
    "So for 170K matches, we are looking at some 5M player records to create. \n",
    "This likely drops again as there will be lots of duplicates, as players are reused.\n",
    "\n",
    "Processing the HTML for 100 matches takes 24 s, \n",
    "Processing the HTML for 1000 matches takes 5 min, \n",
    "for 150K we are at 750 min. So roughly 12 hours.\n",
    "13K 725 min ???\n",
    "\n",
    "After profiling, now 1000 matches takes 60s and scales (down from 5 min, and increasing due to memory problems).\n",
    "The trick is to work with lists, and only in the end convert them to to a pandas dataframe.\n",
    "\n",
    "Expect up to 3 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_player_ids(N):\n",
    "\n",
    "    target = 'raw/df_player_ids_html_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5' # \n",
    "\n",
    "    print(target)\n",
    "\n",
    "    end_match = 4386470 #use from previous cell # \n",
    "    begin_match = 4216257 #\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "    print(n_matches)\n",
    "    full_run = 1\n",
    "\n",
    "    #print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        match_ids = []\n",
    "        player_ids = []\n",
    "        player_numbers = []\n",
    "        player_names = []\n",
    "        \n",
    "        for i in range(n_matches):\n",
    "            match_id = end_match - i\n",
    "\n",
    "            # it spends 99% of the time in this function\n",
    "            match_id_tmp, player_id_tmp, player_number_tmp, player_name_tmp = extract_player_ids_from_single_file(match_id)\n",
    "\n",
    "            match_ids.extend(match_id_tmp) # use extend instead of + for efficiency\n",
    "            player_ids.extend(player_id_tmp)\n",
    "            player_numbers.extend(player_number_tmp)\n",
    "            player_names.extend(player_name_tmp)\n",
    "\n",
    "            if i % 1000 == 0: \n",
    "            # write progress report\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "            if i % 1000 == 0:  \n",
    "            # write data as hdf5 file     \n",
    "                data = zip(match_ids, player_ids, player_numbers, player_names)\n",
    "\n",
    "                df_player_ids_html = pd.DataFrame(data, columns = ['match_id', 'player_id', 'player_number', 'player_name'])\n",
    "\n",
    "                df_player_ids_html.to_hdf(target, key='df_player_ids_html', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(match_ids, player_ids, player_numbers, player_names)\n",
    "\n",
    "        df_player_ids_html = pd.DataFrame(data, columns = ['match_id', 'player_id', 'player_number', 'player_name'])\n",
    "        df_player_ids_html.to_hdf(target, key='df_player_ids_html', mode='w')\n",
    "    else:\n",
    "        print(\"do nothing\")\n",
    "        # read from hdf5 file\n",
    "\n",
    "\n",
    "#extract_player_ids(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n",
    "#%lprun -f extract_player_ids extract_player_ids(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html = pd.read_hdf('raw/df_player_ids_html_20221229_121951.h5')\n",
    "df_player_ids_html.query('match_id == 4353201') # 4353065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "df_player_ids_html[df_player_ids_html.duplicated(['player_id'], keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html.match_id.nunique()\n",
    "\n",
    "df_player_ids_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html = df_player_ids_html.drop_duplicates(subset='player_id', keep='first')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_player_ids_html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the match HTML files\n",
    "\n",
    "I highly recommend [this tutorial](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/) for a great introduction to `BeautifulSoup`.\n",
    "It allows easy access to the information contained within HTML \"div\" tags that partition the web page in different sections. \n",
    "\n",
    "In addition, to clean up the scraped text, I used the **re** Python module (Regular expressions), part of the [Python standard library](https://docs.python.org/3/library/index.html) to extract the actual inducements from the text string that contains them.\n",
    "\n",
    "Processing 1000 matches takes 2 min, so to process all 150K matches is expected to take 300 min (in the end it took 800+ min).\n",
    "\n",
    "Now it takes 1.5 min. 40K matches, expect 1h. check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_stuff():\n",
    "\n",
    "    target = 'raw/df_matches_html_' + time.strftime(\"%Y%m%d_%H%M%S\") + '.h5'\n",
    "\n",
    "    print(target)\n",
    "\n",
    "    end_match = 4429319 \n",
    "    begin_match = 4386470\n",
    "\n",
    "    n_matches = end_match - begin_match\n",
    "    full_run = 0\n",
    "\n",
    "    print(n_matches)\n",
    "\n",
    "    if(full_run):\n",
    "        match_id = [] # used CTRL + D to add to selection\n",
    "        team1_inducements = []\n",
    "        team2_inducements = []\n",
    "        coach1_ranking = []\n",
    "        coach2_ranking = []\n",
    "        team1_comp = []\n",
    "        team2_comp = []\n",
    "        team1_pass = []\n",
    "        team2_pass = []\n",
    "        team1_rush = []\n",
    "        team2_rush = []\n",
    "        team1_block = []\n",
    "        team2_block = []\n",
    "        team1_foul = []\n",
    "        team2_foul = []\n",
    "\n",
    "        for i in range(n_matches):\n",
    "            match_id_tmp = end_match - i\n",
    "            dirname = \"raw/match_html_files/\" + str(match_id_tmp)[0:4]\n",
    "\n",
    "            # PM first check gz, if not exist then check for unzipped html\n",
    "            fname_string = dirname + \"/match_\" + str(match_id_tmp) + \".html\"        \n",
    "            fname_string_gz = dirname + \"/match_\" + str(match_id_tmp) + \".html.gz\"        \n",
    "\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                soup = BeautifulSoup(f, 'xml')\n",
    "\n",
    "            if soup.find(\"div\", {\"class\": \"matchrecord\"}) is not None:\n",
    "                # match record is available\n",
    "                inducements = soup.find_all(\"div\", class_=\"inducements\")\n",
    "\n",
    "                pattern = re.compile(r'\\s+Inducements: (.*)\\n')\n",
    "\n",
    "                match = re.match(pattern, inducements[0].get_text())\n",
    "                if match:\n",
    "                    team1_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team1_inducements_tmp = ''\n",
    "\n",
    "                match = re.match(pattern, inducements[1].get_text())\n",
    "                if match:\n",
    "                    team2_inducements_tmp = match.group(1)\n",
    "                else:\n",
    "                    team2_inducements_tmp = ''\n",
    "\n",
    "                coach_info = soup.find_all(\"div\", class_=\"coach\")\n",
    "                # grab the ranking\n",
    "                coach1_ranking_tmp = coach_info[0].get_text()\n",
    "                coach2_ranking_tmp = coach_info[1].get_text()\n",
    "\n",
    "                # match performance stats\n",
    "                div = soup.find_all('div', class_= \"player foot\")\n",
    "                # passing completions\n",
    "                regex = re.compile('.*front comp statbox.*')\n",
    "                team1_comp_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_comp_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()                   \n",
    "                # passing distance in yards\n",
    "                regex = re.compile('.*back pass statbox.*')\n",
    "                team1_pass_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_pass_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # rushes\n",
    "                regex = re.compile('.*back rush statbox.*')\n",
    "                team1_rush_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_rush_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # block\n",
    "                regex = re.compile('.*back block statbox.*')\n",
    "                team1_block_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_block_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                # foul\n",
    "                regex = re.compile('.*back foul statbox.*')\n",
    "                team1_foul_tmp = div[0].find(\"div\", {\"class\" : regex}).get_text()\n",
    "                team2_foul_tmp = div[1].find(\"div\", {\"class\" : regex}).get_text()\n",
    "\n",
    "                match_id.append(match_id_tmp) # append for single item, extend for multiple items\n",
    "                team1_inducements.append(team1_inducements_tmp)\n",
    "                team2_inducements.append(team2_inducements_tmp)\n",
    "                coach1_ranking.append(coach1_ranking_tmp)\n",
    "                coach2_ranking.append(coach2_ranking_tmp)\n",
    "                team1_comp.append(team1_comp_tmp)\n",
    "                team2_comp.append(team2_comp_tmp)\n",
    "                team1_pass.append(team1_pass_tmp)\n",
    "                team2_pass.append(team2_pass_tmp)\n",
    "                team1_rush.append(team1_rush_tmp)\n",
    "                team2_rush.append(team2_rush_tmp)\n",
    "                team1_block.append(team1_block_tmp)\n",
    "                team2_block.append(team2_block_tmp)\n",
    "                team1_foul.append(team1_foul_tmp)\n",
    "                team2_foul.append(team2_foul_tmp)\n",
    "            \n",
    "            if i % 1000 == 0: \n",
    "            # write progress report\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "\n",
    "                data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                        coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                        team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                        team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "                df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "                'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "                'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "                'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "                df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(match_id, team1_inducements, team2_inducements, \n",
    "                                coach1_ranking, coach2_ranking, team1_comp, team2_comp,\n",
    "                                team1_pass, team2_pass, team1_rush, team2_rush,\n",
    "                                team1_block, team2_block, team1_foul, team2_foul)\n",
    "\n",
    "        df_matches_html = pd.DataFrame(data, columns = ['match_id', 'team1_inducements', 'team2_inducements',\n",
    "        'coach1_ranking', 'coach2_ranking', 'team1_comp', 'team2_comp',\n",
    "        'team1_pass', 'team2_pass', 'team1_rush', 'team2_rush',\n",
    "        'team1_block', 'team2_block', 'team1_foul', 'team2_foul'])\n",
    "\n",
    "        df_matches_html.to_hdf(target, key='df_matches_html', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file    \n",
    "        df_matches_html1 = pd.read_hdf('raw/df_matches_html_20220315_220825.h5') \n",
    "        df_matches_html2 = pd.read_hdf('raw/df_matches_html_20220316_133752.h5') \n",
    "        df_matches_html3 = pd.read_hdf('raw/df_matches_html_20220608_054453.h5')\n",
    "        df_matches_html4 = pd.read_hdf('raw/df_matches_html_20230115_133734.h5')\n",
    "        df_matches_html = pd.concat([df_matches_html1, df_matches_html2, df_matches_html3, df_matches_html4], ignore_index= True)\n",
    "\n",
    "    return df_matches_html\n",
    "\n",
    "df_matches_html = do_all_the_stuff()\n",
    "\n",
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext line_profiler\n",
    "#%lprun -f do_all_the_stuff do_all_the_stuff()\n",
    "# df_matches_html_20230115_133734\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion of the profiling: >95% of the time is spend within Beautiful Soup. Switch to xml parser doubled processing speed.\n",
    "No further room for improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fix the performance stats\n",
    "\n",
    "The completions have a few weird values like `4/1`, we drop the slash and the value behind that.\n",
    "`-` is converted to 0 when the string ends directly after the `-` character, i.e. `-` becomes 0 but `-1` becomes -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['team1_comp'] = df_matches_html['team1_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team1_comp'] = pd.to_numeric(df_matches_html['team1_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team2_comp'] = df_matches_html['team2_comp'].str.replace(r'(/).*','')\n",
    "df_matches_html['team2_comp'] = pd.to_numeric(df_matches_html['team2_comp'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_pass'] = pd.to_numeric(df_matches_html['team1_pass'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_pass'] = pd.to_numeric(df_matches_html['team2_pass'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_rush'] = pd.to_numeric(df_matches_html['team1_rush'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_rush'] = pd.to_numeric(df_matches_html['team2_rush'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_block'] = pd.to_numeric(df_matches_html['team1_block'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_block'] = pd.to_numeric(df_matches_html['team2_block'].str.replace(r'-$','0'))\n",
    "\n",
    "df_matches_html['team1_foul'] = pd.to_numeric(df_matches_html['team1_foul'].str.replace(r'-$','0'))\n",
    "df_matches_html['team2_foul'] = pd.to_numeric(df_matches_html['team2_foul'].str.replace(r'-$','0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: coach rankings\n",
    "\n",
    "We want to extract the part `CR 149.99` from the scraped coach information field (example `test_string` below). Just as we matches on `Inducements:`, we can match on `CR ` and grab the contents directly after that, stopping when we encounter a whitespace.\n",
    "\n",
    "We first play around a bit and test until we discover the proper Regular Expression to use :-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'kingcann Emerging Star CR 149.99 (-0.76)'\n",
    "\n",
    "pattern = re.compile(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "match = re.match(pattern, test_string)\n",
    "\n",
    "if match is not None:\n",
    "    print(match.group(1)) # group(0) is the whole string\n",
    "else:\n",
    "    print(\"match is none\")\n",
    "\n",
    "test_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 'm! Now that we have figured it out, we can write the code that extracts the coach rankings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataprep fix match_id\n",
    "df_matches_html['match_id'] = pd.to_numeric(df_matches_html.match_id) \n",
    "\n",
    "# Dataprep: add the coach rankings as separate cols\n",
    "df_matches_html['coach1_CR'] = df_matches_html['coach1_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "df_matches_html['coach2_CR'] = df_matches_html['coach2_ranking'].str.extract(r'.*CR (.*)\\s\\(.*')\n",
    "\n",
    "df_matches_html['coach1_CR'] = pd.to_numeric(df_matches_html['coach1_CR'])\n",
    "df_matches_html['coach2_CR'] = pd.to_numeric(df_matches_html['coach2_CR'])\n",
    "\n",
    "# abs\n",
    "df_matches_html['CR_diff'] = np.abs(df_matches_html['coach1_CR'] - df_matches_html['coach2_CR'])\n",
    "df_matches_html['CR_diff'] = df_matches_html['CR_diff'].astype(float)\n",
    "\n",
    "# +/-\n",
    "df_matches_html['cr_diff2'] = df_matches_html['coach1_CR'] - df_matches_html['coach2_CR']\n",
    "\n",
    "df_matches_html['cr_diff2_bin'] = pd.cut(df_matches_html['cr_diff2'], bins = [-1*float(\"inf\"), -30, -20, -10, -5, 5, 10, 20, 30, float(\"inf\")], \n",
    " labels=['{-Inf,-30]', '[-30,-20]', '[-20,-10]', '[-10,-5]', '[-5,5]', '[5,10]', '[10,20]', '[20,30]', '[30,Inf]']) \n",
    "\n",
    "df_matches_html['coach1_CR_bin'] = pd.cut(df_matches_html['coach1_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n",
    "\n",
    "df_matches_html['coach2_CR_bin'] = pd.cut(df_matches_html['coach2_CR'], \n",
    "    bins = [0, 135,145, 155, 165, 175, float(\"inf\")], \n",
    "    labels=['CR135-', 'CR140', 'CR150', 'CR160','CR170', 'CR175+'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches_html['coach1_CR_bin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep match inducements for each team\n",
    "\n",
    "The next trick is to use `pandas` `explode()` method (similar to `separate_rows()` in `tidyverse` R) to give each inducement its own row in the dataset.\n",
    "This creates a dataframe (`inducements`) similar to `df_mbt` with each match generating at least two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team1_inducements = df_matches_html[['match_id', 'team1_inducements']]\n",
    "team2_inducements = df_matches_html[['match_id', 'team2_inducements']]\n",
    "\n",
    "# make column names equal\n",
    "team1_inducements.columns = team2_inducements.columns = ['match_id', 'inducements']\n",
    "team1_inducements['team'] = 'team1'\n",
    "team2_inducements['team'] = 'team2'\n",
    "\n",
    "# row bind the two dataframes\n",
    "inducements = pd.concat([team1_inducements, team2_inducements], ignore_index = True)\n",
    "\n",
    "# convert comma separated string to list\n",
    "inducements['inducements'] = inducements['inducements'].str.split(',')\n",
    "\n",
    "# make each element of the list a separate row\n",
    "inducements = inducements.explode('inducements')\n",
    "\n",
    "# strip leading and trailing whitespaces\n",
    "inducements['inducements'] = inducements['inducements'].str.strip()\n",
    "\n",
    "# create \"star player\" label\n",
    "inducements['star_player'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Star player\"), 'star_player'] = 1\n",
    "\n",
    "# create \"card\" label\n",
    "inducements['special_card'] = 0\n",
    "inducements.loc[inducements['inducements'].str.contains(\"Card\"), 'special_card'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inducements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add match HTML data to df_matches\n",
    "\n",
    "Here we add `df_matches_html` to `df_matches`. This contains each players inducements as a single string, not convenient for analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_matches_html, on='match_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add specific inducement info to df_matches\n",
    "\n",
    "The `inducements` dataframe cannot easily be added to `df_matches`. We can however, extract information from `inducements` at the match level and add this to `df_matches`. Here, I show how to add a 1/0 flag `has_sp` that codes for if a match included any star player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sp = (inducements\n",
    "            .groupby(\"match_id\")\n",
    "            .agg(has_sp = (\"star_player\", \"max\"))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "df_matches = pd.merge(df_matches, df_sp, on = \"match_id\", how = \"left\")\n",
    "\n",
    "df_matches['match_id'] = pd.to_numeric(df_matches.match_id) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: API scraping of team data\n",
    "\n",
    "Great! Almost there. There is still something missing though.\n",
    "\n",
    "FUMBBL allows coaches to create their own rulesets to play their own leagues and tournaments with. For example, there is a so-called \"Secret League\" where coaches can play with \"Ninja halflings\", \"Ethereal\" spirits etc. Instead of plain normal regular \"Halflings\" and \"Shambling Undead\" :-)\n",
    "\n",
    "Since we want the team strength for the official rulesets BB2016 and BB2020, we need to distinguish those matches from the matches that are played under different rules.\n",
    "We need to know, for all the matches in our `df_matches` dataset, in what `division` or `league` the match took place, and what version of the rules (encoded in `ruleset`) was used. This information is available in the FUMBBL API, but not on the match level but on the **team** level.\n",
    "\n",
    "So, let us grab for all teams in `df_matches` the team `division`, `division_id`, `league`, and `ruleset` (last two are both numbers).\n",
    "\n",
    "As most other available information through the team API can vary over time (i.e. number of games played, how may rerolls a team has, or whether it has an apothecary), we do no fetch this information (expect number of games played), as we cannot link it to specific matches. We only fetch the information we expect to be valid for **all matches** played by this team. \n",
    "\n",
    "A limitation of the FUMBBL API is that it shows only the latest version of the teams and leagues data.  \n",
    "\n",
    "This hides the fact that leagues have changed their rules since they were first created. For example, the NAF used BB2016 rules up until summer of 2021, and thereafter switched to the new BB2020 ruleset for their latest online tournament.\n",
    "So we have to use our \"domain knowledge\" here to interpret the data properly.\n",
    "\n",
    "**PM we now have tournament id  as well, possibly this allows to at least pinpoint when rulesets might have changed**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "just get all the teams for local storage.\n",
    "PM Next time, write code that checks for presence of local team files, and if not present query API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make list of all teams that need to be fetched\n",
    "#team_ids = list(df_matches['team1_id'].dropna()) + list(df_matches['team2_id'].dropna())\n",
    "\n",
    "all_team_ids = (df_matches\n",
    ".loc[:,['team1_id']]\n",
    ".team1_id.tolist()\n",
    ") +  (df_matches\n",
    ".loc[:,['team2_id']]\n",
    ".team2_id.tolist()\n",
    ")\n",
    "\n",
    "# get unique values by converting to a Python set and back to list\n",
    "all_team_ids = list(set(all_team_ids))\n",
    "\n",
    "len(all_team_ids)\n",
    "\n",
    "team_ids_to_fetch = all_team_ids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first we only get the team data and store it on disk as gzipped JSON\n",
    "\n",
    "each team takes 1 s, so we expect for 20K teams\n",
    "\n",
    "4K per hour, 5 hours.\n",
    "500 min 30K teams. \n",
    "\n",
    "75K teams, took 1.5 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save team API call as gzipped JSON object.\n",
    "# Combine with https://fumbbl.com/api/team/getOptions/1038960\n",
    "\n",
    "print(\"teams to scrape: \")\n",
    "n_teams = len(team_ids_to_fetch)\n",
    "\n",
    "print(n_teams)\n",
    "\n",
    "fullrun = 0\n",
    "\n",
    "if fullrun:\n",
    "    print('fetching team data for ', len(team_ids_to_fetch), ' teams')\n",
    "\n",
    "    for t in range(n_teams):\n",
    "        team_id = int(team_ids_to_fetch[t])\n",
    "            \n",
    "        api_string = \"https://fumbbl.com/api/team/get/\" + str(team_id)\n",
    "\n",
    "        response = requests.get(api_string)\n",
    "        response = response.json()\n",
    "\n",
    "        dirname = \"raw/team_files/\" + str(team_id)[0:4]\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        fname_string = dirname + \"/team_\" + str(team_id) + \".json.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"w\") as f:\n",
    "            f.write(json.dumps(response).encode('utf-8'))  \n",
    "            f.close()\n",
    "\n",
    "        time.sleep(0.3)\n",
    "        # also get tournament skills via getoptions\n",
    "        api_string = \"https://fumbbl.com/api/team/getOptions/\" + str(team_id)\n",
    "        response = requests.get(api_string)\n",
    "        response = response.json()\n",
    "        response['tournamentSkills'] = json.loads(response['tournamentSkills'])\n",
    "\n",
    "        fname_string = dirname + \"/team_\" + str(team_id) + \"_skills.json.gz\"\n",
    "        \n",
    "        with gzip.open(fname_string, mode = \"wb\") as f:\n",
    "            f.write(json.dumps(response).encode(\"utf-8\"))\n",
    "            f.close()\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        if t % 1000 == 0: \n",
    "            # write progress report\n",
    "            print(t, end='')\n",
    "            print(\".\", end='')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## process team data from disk\n",
    "\n",
    "So we have to process data for 5K different teams. \n",
    "\n",
    "We use the same approach as above, now looping over all `team_id` 's\n",
    "\n",
    "PM add all relevant team info that add to TV in tournament (rerolls , teamvalue , fan factor etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all_the_team_stuff():\n",
    "    \n",
    "    n_teams = len(team_ids_to_fetch)\n",
    "\n",
    "    fullrun = 0\n",
    "\n",
    "    if fullrun:\n",
    "        team_id = []\n",
    "        division_id = []\n",
    "        division_name = []\n",
    "        league = []\n",
    "        ruleset = []\n",
    "        roster_id = []\n",
    "        race_name = []\n",
    "        games_played = []\n",
    "\n",
    "        print('fetching team data for ', len(team_ids_to_fetch), ' teams')\n",
    "\n",
    "        for t in range(n_teams):    \n",
    "            team_id = int(team_ids_to_fetch[t])\n",
    "            dirname = \"raw/team_files/\" + str(team_id)[0:4]\n",
    "\n",
    "            fname_string_gz = dirname + \"/team_\" + str(team_id) + \".json.gz\"        \n",
    "            \n",
    "            # PM read compressed json file\n",
    "            with gzip.open(fname_string_gz, mode = \"rb\") as f:\n",
    "                team = json.load(f)\n",
    "\n",
    "            # grab fields\n",
    "            team_id_tmp = team['id']\n",
    "            division_id_tmp = team['divisionId']\n",
    "            division_name_tmp = team['division']\n",
    "            ruleset_tmp = team['ruleset']\n",
    "            league_tmp = team['league']\n",
    "            roster_id_tmp = team['roster']['id']\n",
    "            race_name_tmp = team['roster']['name']\n",
    "            games_played_tmp = team['record']['games']\n",
    "\n",
    "            team_id.append()\n",
    "            division_id.append(division_id_tmp)\n",
    "            division_name.append(division_name_tmp)\n",
    "            league.append(ruleset_tmp)\n",
    "            ruleset.append(league_tmp)\n",
    "            roster_id.append(roster_id_tmp)\n",
    "            race_name.append(race_name_tmp)\n",
    "            games_played.append(games_played_tmp)       \n",
    "            \n",
    "            if t % 100 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(t, end='')\n",
    "                print(\".\", end='')\n",
    "                data = zip(team_id, division_id, division_name, league, ruleset, roster_id, race_name,  games_played)\n",
    "\n",
    "                df_teams = pd.DataFrame(data, columns=['team_id', 'division_id', 'division_name',  'league' ,\n",
    "        'ruleset', 'roster_id', 'race_name',  'games_played'])\n",
    "                df_teams.to_hdf(target, key='df_teams', mode='w')\n",
    "        \n",
    "        data = zip(team_id, division_id, division_name, league, ruleset, roster_id, race_name,  games_played)\n",
    "                \n",
    "        df_teams = pd.DataFrame(data, columns=['team_id', 'division_id', 'division_name',  'league' ,\n",
    "        'ruleset', 'roster_id', 'race_name',  'games_played'])\n",
    "        df_teams.to_hdf(target, key='df_teams', mode='w')\n",
    "\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        df_teams1 = pd.read_hdf('raw/df_teams_20220316_221902.h5')\n",
    "        df_teams2 = pd.read_hdf('raw/df_teams_20220609_062756.h5')\n",
    "        df_teams = pd.concat([df_teams1, df_teams2], ignore_index=True)\n",
    "    return df_teams\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_teams = do_all_the_team_stuff()\n",
    "\n",
    "df_teams['roster_name'] = df_teams['roster_id'].astype(str) + '_' + df_teams['race_name']\n",
    "\n",
    "df_teams.shape\n",
    "\n",
    "df_teams.info()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: Add ruleset_version and division_name to df_teams\n",
    "\n",
    "Lets have look at the various divisions and leagues, and which rulesets are used.\n",
    "There are a lot of small leagues being played on FUMBBL, they account for maybe X% of all the matches.\n",
    "\n",
    "We only look at divisions and leagues with a sufficient volume of matches, or otherwise we do not have sufficient statistics for each race.\n",
    "\n",
    "So I aggregated the data by division, league and ruleset, and filtered on at least 150 different teams that have played at least once last year.\n",
    "Apart from the main \"Divisions\" that are part of FUMBBL, there were a few user-run leagues present in this table, so I looked up their names on FUMBBL and what ruleset is used (BB2016, BB2020 or some other variant). This information (contained in an xlsx) is added to the dataset below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ruleset_version and division_name from codelist CSV\n",
    "ruleset_division_names = pd.read_csv('codelists/ruleset_division_names.csv')\n",
    "\n",
    "# initial creation of the codelist\n",
    "#ruleset_division_names.to_csv('codelists/ruleset_division_names.csv', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "df_teams = pd.merge(df_teams, ruleset_division_names, on= ['league', 'ruleset', 'division_id'], how='left')\n",
    "\n",
    "df_teams['division_name'] = df_teams['new_division_name']\n",
    "\n",
    "df_teams = df_teams.drop('new_division_name', 1)\n",
    "\n",
    "df_teams['division_id'] = pd.to_numeric(df_teams.division_id) \n",
    "df_teams['roster_id'] = pd.to_numeric(df_teams.roster_id) \n",
    "df_teams['team_id'] = pd.to_numeric(df_teams.team_id) \n",
    "df_teams['games_played'] = pd.to_numeric(df_teams.games_played) \n",
    "\n",
    "df_teams['league'] = pd.to_numeric(df_teams.league) \n",
    "df_teams['ruleset'] = pd.to_numeric(df_teams.ruleset) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we have labeled all high volume leagues, divisions and rulesets:\n",
    "\n",
    "**PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310?\n",
    "this has a few changes (tier, gold, crossleague)\n",
    "Do we also see this in the XML API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_teams\n",
    "    .groupby(['ruleset', 'league', 'division_id', 'division_name',  'ruleset_version'], dropna=False)\n",
    "    .agg( n_teams = ('ruleset', 'count')\n",
    "    )\n",
    "    .sort_values('n_teams', ascending = False)\n",
    "    .query('n_teams > 150')['n_teams']\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataprep: Merging the division/ruleset data with the match data\n",
    "\n",
    "As most of the data in `df_teams` is actually on the level of the match, we can merge on `team1_id` after leaving out the team specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matches = pd.merge(df_matches, df_teams.drop(['race_name', 'roster_id', 'roster_name', 'games_played'], 1), left_on='team1_id', right_on = 'team_id', how='left')\n",
    "\n",
    "df_matches['team1_id'] = pd.to_numeric(df_matches.team1_id) \n",
    "df_matches = df_matches.drop('team_id', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Create matches by team DataFrame\n",
    "\n",
    "When analyzing the data, we also like to have a dataframe `df_mbt (df_matches_by_team)` that contains, for each match, a separate row for each team participating in that match.\n",
    "This structure is nicely visualized [at the Nufflytics blog](https://www.nufflytics.com/post/the-value-of-tv/).\n",
    "Such a dataset is suitable for adding, at the match level, data that is specific for each team - coach pair, such as team value, coach rating etc.\n",
    "For example, we can imagine adding more team level data, such as casualties caused during the match, or team composition at the start of the match etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two copies, one for each team in the match\n",
    "team1_data = df_matches[['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team1_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'team1_coach_id', 'team1_race_name', 'team1_value', 'team1_score', 'team1_win', 'tv_diff', 'tv_match', 'team2_coach_id', 'team2_race_name', 'team2_value', 'team2_score', \n",
    "    'team1_comp', 'team1_pass', 'team1_rush', 'team1_block', 'team1_foul', 'team1_cas', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'team2_comp', 'team2_pass', 'team2_rush', 'team2_block', 'team2_foul', 'team2_cas', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach1_CR', 'coach2_CR', 'coach1_CR_bin', 'CR_diff',  'has_sp']].copy()\n",
    "\n",
    "team2_data = df_matches[['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team2_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'team2_coach_id', 'team2_race_name', 'team2_value', 'team2_score', 'team2_win', 'tv_diff', 'tv_match', 'team1_coach_id', 'team1_race_name', 'team1_value', 'team1_score',\n",
    "    'team2_comp', 'team2_pass', 'team2_rush', 'team2_block', 'team2_foul', 'team2_cas', 'team2_cas_bh', 'team2_cas_si', 'team2_cas_rip',\n",
    "    'team1_comp', 'team1_pass', 'team1_rush', 'team1_block', 'team1_foul', 'team1_cas', 'team1_cas_bh', 'team1_cas_si', 'team1_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach2_CR', 'coach1_CR', 'coach2_CR_bin','CR_diff', 'has_sp']].copy()\n",
    "\n",
    "team1_data.columns = team2_data.columns = ['match_id', 'match_date', 'week_number',\t'year',\t'week_year', 'week_date', 'team_id', 'ruleset', 'league', 'division_id', 'division_name', 'ruleset_version',\n",
    "    'coach_id', 'race_name', 'team_value', 'team_score', 'wins', 'tv_diff', 'tv_match',  'away_coach_id', 'away_race_name', 'away_team_value', 'away_team_score',\n",
    "    'home_comp', 'home_pass', 'home_rush', 'home_block', 'home_foul',  'home_cas', 'home_cas_bh', 'home_cas_si', 'home_cas_rip',\n",
    "    'away_comp', 'away_pass', 'away_rush', 'away_block', 'away_foul',  'away_cas', 'away_cas_bh', 'away_cas_si', 'away_cas_rip',\n",
    "    'tv_bin', 'tv_bin2', 'mirror_match', 'coach_CR', 'away_coach_CR', 'coach_CR_bin','CR_diff', 'has_sp']\n",
    "\n",
    "# combine both dataframes\n",
    "df_mbt = pd.concat([team1_data, team2_data])\n",
    "\n",
    "df_mbt['tv_diff2'] = df_mbt['team_value'] - df_mbt['away_team_value']\n",
    "df_mbt['cr_diff2'] = df_mbt['coach_CR'] - df_mbt['away_coach_CR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add games played for each team\n",
    "\n",
    "df_mbt = pd.merge(df_mbt, df_teams[['team_id', 'games_played']], left_on='team_id', right_on = 'team_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding outcome weights\n",
    "\n",
    "One way to measure team strength is to calculate a win rate.\n",
    "If we want to calculate win rates, we need to decide how to weigh a draw.\n",
    "In Blood Bowl data analysis, it seems that a 2:1:0 (W / D / L) weighting scheme is most commonly used. \n",
    "So if we want to compare with others, it makes sense to adapt this scheme as well.\n",
    "If we divide these weights by two we get something that, if we average it, we can interpret as a win rate.\n",
    "\n",
    "This scheme has the advantage that the weighted average win percentage over all matches is always 50%, creating a nice reference point allowing conclusions such as \"this and that team has an x percent above average win percentage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbt.loc[df_mbt['wins'] == 0, 'wins'] = 0.5\n",
    "df_mbt.loc[df_mbt['wins'] == -1, 'wins'] = 0\n",
    "\n",
    "# convert to float\n",
    "df_mbt['wins'] = df_mbt['wins'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, Lets have a look at our dataset again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mbt.query(\"coach_id == 255851\").sort_values('match_date')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: adding race classifications to df_mbt (team tiers, bash/dash/hybrid)\n",
    "\n",
    "There are team classifications for strength (tiers) and classifications that focus on play style (Bash/Dash/Hybrid).\n",
    "The Bash/Dash/Hybrid classification i use is based on this great [Nufflytics blog](https://www.nufflytics.com/post/bash-dash-hybrid-by-the-numbers/), and my own subjective choices.\n",
    "Here we add both to the `df_mbt` dataframe, as in this dataset each row is about a single team, instead of a pairing.\n",
    "\n",
    "According to [this article from the NAF from 2017](https://www.thenaf.net/2017/05/tiers/), already since 2010 efforts were made to balance things out a bit between the different team strengths. For example, the weaker teams get more gold to spend on players, or get more so-called \"Star player points\" to spend on skilling players up. \n",
    "\n",
    "According to [the NAF](https://www.thenaf.net/tournaments/information/tiers-and-tiering/), traditionally team tiering consists of three groups, with Tier 1 being the strongest teams, and tier 3 the weakest teams. \n",
    "\n",
    "The GW BB2020 rule book also contains three tier groups, that are similar to the NAF tiers: except for Humans and Old World Alliance. \n",
    "\n",
    "And in november 2021, Games Workshop published an update of the three tier groups, now with High Elves moving from tier 2 to tier 1, and Old World Alliance moving back to tier 2.\n",
    "\n",
    "Finally, I added an additional race classification into the well known bash/dash (agile)/hybrid / stunty classes.\n",
    "\n",
    "**PM Vampires is on other**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_tiers = pd.read_csv('codelists/race_tiers_mapping.csv')\n",
    "#race_tiers = race_tiers[ ['race_name', 'bb2020_tier', 'naf_tier', 'bb2020_nov21_tier', 'race_type']]\n",
    "#race_tiers = race_tiers.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bb2020 tiers\n",
    "df_mbt = pd.merge(df_mbt, race_tiers, on='race_name', how='left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2b: API scraping the player data: df_players\n",
    "\n",
    "Now that we have for all matches the player_id's involved, we fetch the player data.\n",
    "Were mostly interested in the player position, so we can reconstruct the team roster for a specific match.\n",
    "We can also analyze the development of teams over time: which positionals are bought in what order?\n",
    "\n",
    "1K players takes 10 min, so 4K players 40 min.\n",
    "\n",
    "We use the API, as it contains identical info as the HTML page.\n",
    "\n",
    "https://fumbbl.com/api/player/get/13524599\n",
    "\n",
    "PM Need to add the match_id \n",
    "so we are facing 1M players to scrape.\n",
    "at a rate of 2 scrapes per second, this will take 6 days of permanent scraping.\n",
    "We'll subset on BB2020, this will take 3 days.\n",
    "170K matches takes 8gb of disk space.\n",
    "check filesize of a single player json object. Maybe compress?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def write_json_file(json_object, player_id):\n",
    "\n",
    "    dirname = \"raw/player_html_files/\" + str(player_id)[0:4]\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    fname_string = dirname + \"/player_\" + str(player_id) + \".html\"\n",
    "\n",
    "    with open(fname_string, mode = \"w\", encoding='UTF-8') as f:\n",
    "        f.write(json.dumps(json_object, ensure_ascii=False, indent=4))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id = 14693304\n",
    "\n",
    "api_string = \"https://fumbbl.com/api/player/get/\" + str(player_id)\n",
    "\n",
    "player = requests.get(api_string)\n",
    "# PM here save JSON as file\n",
    "player = player.json()\n",
    "\n",
    "write_json_file(player, player_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_player_ids_html needed in bg\n",
    "def fetch_player_data_from_api(n_players):\n",
    "    full_run = 1\n",
    "    \n",
    "    if n_players > len(df_player_ids_html):\n",
    "        n_players = len(df_player_ids_html)\n",
    "    print(n_players)\n",
    "\n",
    "    for i in range(n_players):\n",
    "        player_id = df_player_ids_html.iloc[i].player_id\n",
    "        api_string = \"https://fumbbl.com/api/player/get/\" + str(player_id)\n",
    "        # PM add exception handling\n",
    "        player = requests.get(api_string)\n",
    "        # PM here save JSON as file\n",
    "        player = player.json()\n",
    "        write_json_file(player, player_id)\n",
    "        \n",
    "        if i % 100 == 0: \n",
    "            #show progress\n",
    "            print(i, end='')\n",
    "            print(\".\", end='')\n",
    "\n",
    "    return print(i+1, \"files written\")\n",
    "\n",
    "fetch_player_data_from_api(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the player JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_json_file(player_id):\n",
    "\n",
    "    dirname = \"raw/player_html_files/\" + str(player_id)[0:4]\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "\n",
    "    fname_string = dirname + \"/player_\" + str(player_id) + \".html\"\n",
    "\n",
    "    with open(fname_string, mode = \"r\", encoding='UTF-8') as f:\n",
    "        json_object = json.load(f)\n",
    "\n",
    "    return json_object\n",
    "\n",
    "player = read_json_file(14693304)\n",
    "player['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to fetch the player position to check what skills are added.\n",
    "https://fumbbl.com/api/position/get/39328\n",
    "But this is the current set of skills, not necessary what skills were used in a particular game.\n",
    "\n",
    "For now we just go for position plots for say team values around 1200K.\n",
    "This will not work because the 1200K includes skills and we do not know them.\n",
    "\n",
    "Maybe check for tournaments with fixed skill choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_player_data_from_file(n_players):\n",
    "    target = 'raw/df_players_' + 'tst' + '.h5' # time.strftime(\"%Y%m%d_%H%M%S\") \n",
    "    print(target)\n",
    "\n",
    "    full_run = 1\n",
    "    \n",
    "    if n_players > len(df_player_ids_html):\n",
    "        n_players = len(df_player_ids_html)\n",
    "    print(n_players)\n",
    "\n",
    "    if(full_run):\n",
    "        player_ids = []\n",
    "        team_id = []\n",
    "        status = []\n",
    "        number = []\n",
    "        name = []\n",
    "        position_id = []\n",
    "        position_name = []\n",
    "        stats_ma = []\n",
    "        stats_st = []\n",
    "        stats_ag = []\n",
    "        stats_pa = []\n",
    "        stats_av = []\n",
    "\n",
    "        for i in range(n_players):\n",
    "            player_id = df_player_ids_html.iloc[i].player_id\n",
    "\n",
    "            player = read_json_file(player_id)\n",
    "            \n",
    "            if player: # fix for matches that do not exist\n",
    "                player_ids.append(player['id'])\n",
    "                team_id.append(player['teamId'])\n",
    "                status.append(player['status'])\n",
    "                number.append(player['number'])\n",
    "                name.append(player['name'])\n",
    "                position_id.append(player['position']['id'])\n",
    "                position_name.append(player['position']['name'])\n",
    "                stats_ma.append(player['stats']['ma'])\n",
    "                stats_st.append(player['stats']['st'])\n",
    "                stats_ag.append(player['stats']['ag'])\n",
    "                stats_pa.append(player['stats']['pa'])\n",
    "                stats_av.append(player['stats']['av'])\n",
    "                # PM skills, injuries: add in for loop (variable length)\n",
    "\n",
    "            else:\n",
    "                # empty data for this match, create empty row\n",
    "                print('An error has occurred.')\n",
    "                #df_players.loc[i] = np.repeat([np.NaN], 12, axis=0)\n",
    "                #df_players.loc[i]['player_id'] = int(player_id)\n",
    "            \n",
    "            if i % 100 == 0: \n",
    "                # write tmp data as hdf5 file\n",
    "                print(i, end='')\n",
    "                print(\".\", end='')\n",
    "                \n",
    "                data = zip(player_ids, team_id, status, number, name, \n",
    "                position_id, position_name,\n",
    "                stats_ma, stats_st, stats_ag, stats_pa,  stats_av)\n",
    "\n",
    "                df_players = pd.DataFrame(data, columns = ['player_id', 'team_id', 'status', 'number', 'name', \n",
    "                'position_id', 'position_name',\n",
    "                'stats_ma', 'stats_st', 'stats_ag', 'stats_pa',  'stats_av'\n",
    "                ])\n",
    "                df_players.to_hdf(target, key='df_players', mode='w')\n",
    "\n",
    "        # write data as hdf5 file\n",
    "        data = zip(player_ids, team_id, status, number, name, \n",
    "                position_id, position_name,\n",
    "                stats_ma, stats_st, stats_ag, stats_pa,  stats_av)\n",
    "\n",
    "        df_players = pd.DataFrame(data, columns = ['player_id', 'team_id', 'status', 'number', 'name', \n",
    "                'position_id', 'position_name',\n",
    "                'stats_ma', 'stats_st', 'stats_ag', 'stats_pa',  'stats_av'\n",
    "                ])\n",
    "        df_players.to_hdf(target, key='df_players', mode='w')\n",
    "    else:\n",
    "        # read from hdf5 file\n",
    "        print(\"do nothing\")\n",
    "    return df_players\n",
    "\n",
    "df_players = process_player_data_from_file(10)\n",
    "df_players = df_players.sort_values(by=['team_id']).reset_index(drop=True)\n",
    "df_players.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all prepped datasets as HDF5 and CSV files\n",
    "\n",
    "HD5 files can be read by both Python and [R](https://cran.r-project.org/web/packages/hdf5r/index.html) and preserve column data types.\n",
    "\n",
    "**Update** the HDF5 schema used by pandas is highly specific to pandas, it is not designed for external use. An R code snippet is available but does not work with table format. For now using the CSVs for R is advised\n",
    "\n",
    "CSV files are the lingua franca across all data analysis software.\n",
    "\n",
    "A dataset release consists of three datasets:\n",
    "* A list of matches, identified by match_id\n",
    "* A list of matches by team, identified by match_id and team_id\n",
    "* A list of inducements by match_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this to locate variables that cannot be serialized by hdf5\n",
    "#df_matches.loc[:, :'week_date'].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_matches'\n",
    "\n",
    "df_matches.to_hdf(target + '.h5', key='df_matches', mode='w', format = 't',  complevel = 9)\n",
    "df_matches.to_csv(target + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/df_mbt'\n",
    "\n",
    "df_mbt.to_hdf(target + '.h5', key='df_mbt', mode='w', format = 't',  complevel = 9)\n",
    "df_mbt.to_csv(target + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'datasets/current/inducements'\n",
    "inducements.to_hdf(target + '.h5', key='inducements', mode='w', format = 't',  complevel = 9)\n",
    "inducements.to_csv(target + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing a license for the public dataset\n",
    "\n",
    "An important part of making data publicly available is being explicit about what is allowed if people want to use the dataset.\n",
    "However, before we do so, we have to check if **we** are actually allowed to publish the data. This is explained nicely [in a blogpost by Elizabeth Wickes](https://datacarpentry.org/blog/2016/06/data-licensing).\n",
    "\n",
    "Since our data comes from the **FUMBBL.com** website, we check the [**Privacy policy**](https://fumbbl.com/p/privacy) where all users, including myself have agreed on when signing up. It contains this part which is specific to the unauthenticated API, which we use to fetch the data, as well as additional public match data, such as which inducements are used in a match, and the Coach rankings of the playing coaches that were current when the match was played.\n",
    "\n",
    "```\n",
    "Content you provide through the website\n",
    "All the information you provide through the website is processed by FUMBBL. This includes things such as forum posts, private message posts, blog entries, team and player names and biographies and news comments. Data provided this way is visible by other people on the website and in most cases public even to individuals without accounts (not including private messages), and as such are considered of public interest. If direct personal information is posted in public view, you can contact moderators to resolve this. Match records are also considered content in this context, and is also considered of public interest. This data is collected as the primary purpose of the website and it is of course entirely up to you how much of this is provided to FUMBBL. \n",
    "\n",
    "Third party sharing\n",
    "Some of the public data is available through a public (*i.e. unauthenticated*) API, which shares some of the information provided by FUMBBL users in a way suitable for third-party websites and services to process.\n",
    "\n",
    "The data available through the unauthenticated API is considered non-personal as it only reflects information that is public by its nature on the website. The authenticated API will only show information connected to the authenticated account.\n",
    "```\n",
    "\n",
    "I conclude that since the match data is already considered public content, there is no harm in collecting this public data in a structured dataset and placing this data in a public repository. I also verified this with Christer, the site owner. \n",
    "\n",
    "\n",
    "The final step is then to decide what others are allowed to do with this data. In practice, this means choosing a license under which to release the dataset. I decided to choose a CC0 license: this places the data in the public domain, and people can use the dataset as they wish. Citing or mentioning the source of the data would still be appreciated of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List of possible future improvements\n",
    "\n",
    "* Scraping the players (only most recent version, so no player development history)\n",
    "* Scraping the rulesets (for example to identify resurrection tournaments where players choose skills and use tiers)\n",
    "* Switch to feather or Parquet dataformat\n",
    "* catch exception: **PM we cannot deal yet with the situation HTTPSConnectionPool(host='fumbbl.com', port=443): Max retries exceeded with url: /api/match/get/4221820 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4acff12be0>: Failed to establish a new connection: [Errno 110] Connection timed out',))**\n",
    "* PM we now have tournament id  as well, possibly this allows to at least pinpoint when rulesets might have changed\n",
    "* PM we see that (NAF) matches played previously under ruleset 2228 are now labeled as ruleset 2310? this has a few changes (tier, gold, crossleague)\n",
    "* Do we also see this in the XML API\n",
    "* cr_bin variable is gone?\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50276fd1884268afe39607052f22ef19b84d915691d702a5c7e9a67a09867105"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('requests_env': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
